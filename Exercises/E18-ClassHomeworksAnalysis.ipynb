{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 18\n",
    "\n",
    "## Analyze class homeworks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import textdistance\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sexo</th>\n",
       "      <th>T1</th>\n",
       "      <th>T2</th>\n",
       "      <th>T3</th>\n",
       "      <th>T4</th>\n",
       "      <th>T5</th>\n",
       "      <th>T6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H</td>\n",
       "      <td>Un científico de datos y un ingeniero de datos...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Los árboles de decisión es uno de los algoritm...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>En el documento \"Do We Need Hundreds of Classi...</td>\n",
       "      <td>Boosting builds models from individual so call...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H</td>\n",
       "      <td>Aunque los dos perfiles cuentan con habilidade...</td>\n",
       "      <td>Aunque el conceso no es total en el tema de ma...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>En Machine Learing el propósito de los métodos...</td>\n",
       "      <td>El paper publicado en 2014 hace una evaluación...</td>\n",
       "      <td>Gradient Boosting ClassifierGradient Boosting ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>H</td>\n",
       "      <td>Python vs Stata. Analizar información es un pr...</td>\n",
       "      <td>En el marco del Machine Learning encontramos c...</td>\n",
       "      <td>Árbol de clasificación Como su nombre lo indic...</td>\n",
       "      <td>Este es un mecanismo que utiliza varios algori...</td>\n",
       "      <td>El paper analiza 179 clasificadores de 17 dife...</td>\n",
       "      <td>Gradient Boosting Classifier busca aprender de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>Los algoritmos de machine learning buscan pred...</td>\n",
       "      <td>Los algoritmos de machine learning buscan pred...</td>\n",
       "      <td>Los arboles de decisión pertenecen a los algor...</td>\n",
       "      <td>Consideremos el siguiente ejemplo: Una persona...</td>\n",
       "      <td>¿Se necesitan cientos de modelos para resolver...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H</td>\n",
       "      <td>Python vs R para análisis de datos. A pesar de...</td>\n",
       "      <td>Tipos de aprendizaje en Machine Learning. El M...</td>\n",
       "      <td>Tipos de árboles de decisión y aplicaciones. U...</td>\n",
       "      <td>La combinación de modelos o ensemble es un mod...</td>\n",
       "      <td>En el articulo se analizan cerca de 180 clasif...</td>\n",
       "      <td>XGBoosting vs GBoosting. A pesar de que el alg...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sexo                                                 T1  \\\n",
       "0    H  Un científico de datos y un ingeniero de datos...   \n",
       "1    H  Aunque los dos perfiles cuentan con habilidade...   \n",
       "2    H  Python vs Stata. Analizar información es un pr...   \n",
       "3    M  Los algoritmos de machine learning buscan pred...   \n",
       "4    H  Python vs R para análisis de datos. A pesar de...   \n",
       "\n",
       "                                                  T2  \\\n",
       "0                                                NaN   \n",
       "1  Aunque el conceso no es total en el tema de ma...   \n",
       "2  En el marco del Machine Learning encontramos c...   \n",
       "3  Los algoritmos de machine learning buscan pred...   \n",
       "4  Tipos de aprendizaje en Machine Learning. El M...   \n",
       "\n",
       "                                                  T3  \\\n",
       "0  Los árboles de decisión es uno de los algoritm...   \n",
       "1                                                NaN   \n",
       "2  Árbol de clasificación Como su nombre lo indic...   \n",
       "3  Los arboles de decisión pertenecen a los algor...   \n",
       "4  Tipos de árboles de decisión y aplicaciones. U...   \n",
       "\n",
       "                                                  T4  \\\n",
       "0                                                NaN   \n",
       "1  En Machine Learing el propósito de los métodos...   \n",
       "2  Este es un mecanismo que utiliza varios algori...   \n",
       "3  Consideremos el siguiente ejemplo: Una persona...   \n",
       "4  La combinación de modelos o ensemble es un mod...   \n",
       "\n",
       "                                                  T5  \\\n",
       "0  En el documento \"Do We Need Hundreds of Classi...   \n",
       "1  El paper publicado en 2014 hace una evaluación...   \n",
       "2  El paper analiza 179 clasificadores de 17 dife...   \n",
       "3  ¿Se necesitan cientos de modelos para resolver...   \n",
       "4  En el articulo se analizan cerca de 180 clasif...   \n",
       "\n",
       "                                                  T6  \n",
       "0  Boosting builds models from individual so call...  \n",
       "1  Gradient Boosting ClassifierGradient Boosting ...  \n",
       "2  Gradient Boosting Classifier busca aprender de...  \n",
       "3                                                NaN  \n",
       "4  XGBoosting vs GBoosting. A pesar de que el alg...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_excel('E18.xlsx')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "The missing values will be replace by an empty text `\"\"` so this will not create false bais for the writing patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T1</th>\n",
       "      <th>T2</th>\n",
       "      <th>T3</th>\n",
       "      <th>T4</th>\n",
       "      <th>T5</th>\n",
       "      <th>T6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Un científico de datos y un ingeniero de datos...</td>\n",
       "      <td></td>\n",
       "      <td>Los árboles de decisión es uno de los algoritm...</td>\n",
       "      <td></td>\n",
       "      <td>En el documento \"Do We Need Hundreds of Classi...</td>\n",
       "      <td>Boosting builds models from individual so call...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aunque los dos perfiles cuentan con habilidade...</td>\n",
       "      <td>Aunque el conceso no es total en el tema de ma...</td>\n",
       "      <td></td>\n",
       "      <td>En Machine Learing el propósito de los métodos...</td>\n",
       "      <td>El paper publicado en 2014 hace una evaluación...</td>\n",
       "      <td>Gradient Boosting ClassifierGradient Boosting ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Python vs Stata. Analizar información es un pr...</td>\n",
       "      <td>En el marco del Machine Learning encontramos c...</td>\n",
       "      <td>Árbol de clasificación Como su nombre lo indic...</td>\n",
       "      <td>Este es un mecanismo que utiliza varios algori...</td>\n",
       "      <td>El paper analiza 179 clasificadores de 17 dife...</td>\n",
       "      <td>Gradient Boosting Classifier busca aprender de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Los algoritmos de machine learning buscan pred...</td>\n",
       "      <td>Los algoritmos de machine learning buscan pred...</td>\n",
       "      <td>Los arboles de decisión pertenecen a los algor...</td>\n",
       "      <td>Consideremos el siguiente ejemplo: Una persona...</td>\n",
       "      <td>¿Se necesitan cientos de modelos para resolver...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Python vs R para análisis de datos. A pesar de...</td>\n",
       "      <td>Tipos de aprendizaje en Machine Learning. El M...</td>\n",
       "      <td>Tipos de árboles de decisión y aplicaciones. U...</td>\n",
       "      <td>La combinación de modelos o ensemble es un mod...</td>\n",
       "      <td>En el articulo se analizan cerca de 180 clasif...</td>\n",
       "      <td>XGBoosting vs GBoosting. A pesar de que el alg...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  T1  \\\n",
       "0  Un científico de datos y un ingeniero de datos...   \n",
       "1  Aunque los dos perfiles cuentan con habilidade...   \n",
       "2  Python vs Stata. Analizar información es un pr...   \n",
       "3  Los algoritmos de machine learning buscan pred...   \n",
       "4  Python vs R para análisis de datos. A pesar de...   \n",
       "\n",
       "                                                  T2  \\\n",
       "0                                                      \n",
       "1  Aunque el conceso no es total en el tema de ma...   \n",
       "2  En el marco del Machine Learning encontramos c...   \n",
       "3  Los algoritmos de machine learning buscan pred...   \n",
       "4  Tipos de aprendizaje en Machine Learning. El M...   \n",
       "\n",
       "                                                  T3  \\\n",
       "0  Los árboles de decisión es uno de los algoritm...   \n",
       "1                                                      \n",
       "2  Árbol de clasificación Como su nombre lo indic...   \n",
       "3  Los arboles de decisión pertenecen a los algor...   \n",
       "4  Tipos de árboles de decisión y aplicaciones. U...   \n",
       "\n",
       "                                                  T4  \\\n",
       "0                                                      \n",
       "1  En Machine Learing el propósito de los métodos...   \n",
       "2  Este es un mecanismo que utiliza varios algori...   \n",
       "3  Consideremos el siguiente ejemplo: Una persona...   \n",
       "4  La combinación de modelos o ensemble es un mod...   \n",
       "\n",
       "                                                  T5  \\\n",
       "0  En el documento \"Do We Need Hundreds of Classi...   \n",
       "1  El paper publicado en 2014 hace una evaluación...   \n",
       "2  El paper analiza 179 clasificadores de 17 dife...   \n",
       "3  ¿Se necesitan cientos de modelos para resolver...   \n",
       "4  En el articulo se analizan cerca de 180 clasif...   \n",
       "\n",
       "                                                  T6  \n",
       "0  Boosting builds models from individual so call...  \n",
       "1  Gradient Boosting ClassifierGradient Boosting ...  \n",
       "2  Gradient Boosting Classifier busca aprender de...  \n",
       "3                                                     \n",
       "4  XGBoosting vs GBoosting. A pesar de que el alg...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = data.drop('Sexo', axis=1)\n",
    "\n",
    "df.fillna(\"\", inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 18.1\n",
    "\n",
    "Analyze the writing patterns of each student\n",
    "_____\n",
    "\n",
    "Will be obtain the following metrics:\n",
    "\n",
    "* Not delivered text (aka *missing values*)\n",
    "* Average of words used per text\n",
    "* Average of characters used per text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Missing_values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Missing_values\n",
       "0             2.0\n",
       "1             1.0\n",
       "2             0.0\n",
       "3             1.0\n",
       "4             0.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msvl = []\n",
    "\n",
    "for i in np.arange(df.shape[0]):\n",
    "    msvl.append(float(data.loc[i].isna().sum()))\n",
    "\n",
    "msvl = pd.DataFrame(msvl, columns=['Missing_values'])\n",
    "msvl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1845.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1320.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1772.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1467.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1670.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Words\n",
       "0  1845.0\n",
       "1  1320.0\n",
       "2  1772.0\n",
       "3  1467.0\n",
       "4  1670.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aux_words = pd.DataFrame(index = range(0,df.shape[0]), columns=range(0,df.shape[1]), dtype='float')\n",
    "\n",
    "for i in np.arange(df.shape[0]):\n",
    "    count = 0\n",
    "    for j in np.arange(df.shape[1]):\n",
    "        aux_words.iloc[i][j] = len(df.iloc[i][j].split())\n",
    "        \n",
    "words = aux_words.sum(axis=1)\n",
    "words = pd.DataFrame(words, columns=['Words'])\n",
    "words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Characters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11574.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8513.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11102.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9586.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10241.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Characters\n",
       "0     11574.0\n",
       "1      8513.0\n",
       "2     11102.0\n",
       "3      9586.0\n",
       "4     10241.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aux_characters = pd.DataFrame(index = range(0,df.shape[0]), columns=range(0,df.shape[1]), dtype='float')\n",
    "\n",
    "for i in np.arange(df.shape[0]):\n",
    "    count = 0\n",
    "    for j in np.arange(df.shape[1]):\n",
    "        aux_characters.iloc[i][j] = len(df.iloc[i][j])\n",
    "        \n",
    "characters = aux_characters.sum(axis=1)\n",
    "characters = pd.DataFrame(characters, columns=['Characters'])\n",
    "characters.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>average_Words</th>\n",
       "      <th>average_Characters</th>\n",
       "      <th>Missing_values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>461.250000</td>\n",
       "      <td>2893.500000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>264.000000</td>\n",
       "      <td>1702.600000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>295.333333</td>\n",
       "      <td>1850.333333</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>293.400000</td>\n",
       "      <td>1917.200000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>278.333333</td>\n",
       "      <td>1706.833333</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   average_Words  average_Characters  Missing_values\n",
       "0     461.250000         2893.500000             2.0\n",
       "1     264.000000         1702.600000             1.0\n",
       "2     295.333333         1850.333333             0.0\n",
       "3     293.400000         1917.200000             1.0\n",
       "4     278.333333         1706.833333             0.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame(index = range(0, df.shape[0]), columns = range(0,2))\n",
    "results.rename(columns = {0 : 'average_Words',\n",
    "                           1 : 'average_Characters'}, inplace=True)\n",
    "\n",
    "for i in np.arange(results.shape[0]):\n",
    "    results.iloc[i][0] = float (words.iloc[i][0] / (6 - msvl.iloc[i][0]))\n",
    "    results.iloc[i][1] = float(characters.iloc[i][0] / (6 - msvl.iloc[i][0]))\n",
    "    \n",
    "results = pd.concat([results, msvl], axis=1, sort=False)\n",
    "results = results.convert_objects(convert_numeric=True)\n",
    "\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 540x540 with 12 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "sns.pairplot(results)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 18.2\n",
    "\n",
    "Evaluate the similarities of the homeworks of the students\n",
    "\n",
    "tip: https://github.com/orsinium/textdistance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It requires to have a single text for the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux = df['T1'] + \" \" + df['T2'] + \" \" + df['T3'] + \" \" + df['T4'] + \" \" + df['T5'] + \" \" + df['T6']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, makes a test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Un científico de datos y un ingeniero de datos tienen ciertas habilidades en común, por ejemplo, poseen habilidades de programación. Sin embargo, las habilidades de programación de un ingeniero de datos están mucho más allá de las habilidades de programación de un científico de datos. Hacer que un científico de datos cree un flujo de datos está en el extremo más alejado de sus habilidades, pero es el pan de cada día de un ingeniero de datos. En general, existen varias diferencias entre un científico e ingeniero de datos. Un científico de datos puede organizar y almacenar gran cantidad de datos. Además, puede realizar análisis descriptivos, predictivos y prescriptivos, con el fin de resolver una necesidad de negocio y generar valor a la compañía. Dentro de sus áreas de conocimiento están las matemáticas, la estadística, la física, la investigación de operaciones y las ciencias de la computación. Un científico de datos es responsable de optimizar el rendimiento de un modelo de aprendizaje automático o estadístico. Los lenguajes que usualmente utiliza un científico de datos son Python, R, SAS, SPSS, Matlab, Julia, entre otros. También, de acuerdo a Glassdoor, los ingresos anuales de un científico de datos en Estados Unidos están entre 101 y 183 mil dólares, con un promedio de 140 mil dólares. Por su parte, un ingeniero de datos construye, desarrolla, prueba y mantiene arquitecturas, tales como bases de datos y sistemas de procesamiento de larga escala. También, dentro de sus responsabilidades están descubrir oportunidades para adquisición de datos, desarrollar procesos de unificación de bases de datos para modelamiento, minería y producción, y recomendar formas para mejorar la confianza, eficiencia y calidad de los datos. Un ingeniero de datos requiere conocimientos de programación, middleware y hardware. Además, es responsable del rendimiento óptimo de toda una tubería de datos. Los lenguajes, herramientas y softwares que frecuentemente utiliza un ingeniero de datos son SAP, ORACLE, MySQL, redis, riak, neo4j, mongoDB, Java, C, entre otros. Por último, de acuerdo a Glassdoor, los ingresos anuales de un ingeniero de datos en Estados Unidos están entre 123 y 170 mil dólares, con un promedio de 151 mil dólares.  Los árboles de decisión es uno de los algoritmos de clasificación y predicción más utilizados, corresponden a la familia de algoritmos de aprendizaje supervisado, y son los CART (Classification and Regression Tree) los tipos de árboles más conocidos. Los árboles de clasificación tienen como resultado una clase a la cual va a pertenecer la observación, mientras que los árboles de predicción, tienen como resultado un número real (el precio de un producto, el número de bicicletas alquiladas en un rango horario o el tiempo de recuperación de una enfermedad). Un árbol de decisión es una estructura de árbol similar a un diagrama de flujo, donde un nodo interno representa una característica (atributo o variable), la rama representa una regla de decisión y cada nodo terminal representa un resultado. El nodo superior en un árbol de decisión se conoce como el nodo raíz. Un árbol de decisión funciona de la siguiente forma: primero, se ubica el mejor atributo de la base de datos en la raíz del árbol; segundo, se parte la base de datos dentro de subconjuntos. Los subconjuntos deberían ser creados de tal forma que cada uno contiene información con el mismo valor para un atributo. Por último, se deben repetir los pasos uno y dos en cada subconjunto hasta que se llegue a un nodo terminal en todas las ramas del árbol. Un primer reto al implementar un árbol de decisión es identificar cuál atributo debe estar en la raíz del árbol y en cada nivel del mismo. Para identificar esto se utilizan usualmente dos métricas, a saber, el índice Gini y la ganancia de información. Estos criterios calcularán valores para cada atributo. Los valores se ordenan y los atributos se colocan en el árbol siguiendo el orden, es decir, el atributo con un valor alto (en caso de ganancia de información) se coloca en la raíz. En particular, el índice Gini es una métrica para medir la frecuencia con la que un elemento elegido al azar se identifica incorrectamente. Significa que debe preferirse una variable o atributo con un índice de Gini más bajo. Respecto a los tipos de algoritmos de árboles de decisión, aparte de los CART, hay otras especificaciones, como el C4.5, el CHAID y los árboles de inferencia condicionales. Los árboles de decisión generados por C4.5 pueden ser usados para clasificación, y por esta razón, C4.5 está casi siempre referido como un clasificador estadístico. C4.5 construye árboles de decisión desde un grupo de datos de entrenamiento, usando el concepto de entropía de información. Los datos de entrenamiento son un grupo de observaciones ya clasificadas. Cada observación es un vector que representa los atributos o características de la observación. Los datos de entrenamiento son aumentados con un vector c1,c2,…,cn donde c1,c2,..,cn  representan la clase a la que pertenece cada muestra. En cada nodo del árbol, C4.5 elige un atributo que más eficazmente divida el conjunto de muestras en subconjuntos enriquecidos en una clase u otra. Su criterio es la ganancia de información normalizada. El atributo con la mayor ganancia de información normalizada se elige como parámetro de decisión. Por su parte, el CHAID (CHi-squared Automatic Interaction Detector), realiza divisiones multinivel al calcular árboles de clasificación. CHAID se usa a menudo en marketing para seleccionar grupos de consumidores y predecir cómo sus respuestas a algunas variables afectan a otras variables, aunque otras aplicaciones tempranas fueron en el campo de la investigación médica y psiquiátrica. Por último, los árboles de inferencia condicional, es un enfoque que utiliza pruebas no paramétricas como criterios de división, corregido para múltiples pruebas para evitar el sobreajuste. Este enfoque selecciona predictores de forma insesgada.   En el documento \"Do We Need Hundreds of Classifiers to Solve Real World Classification Problems?\" los autores evalúan 179 clasificadores, de 17 familias y en 121 bases de datos, implementados en Weka, R, C/C++ y Matlab, con el fin de determinar cuáles clasificadores tienen mejor rendimiento. Para llevar a cabo esta tarea, los autores generan el training y el test set de forma aleatoria. Luego, los parámetros son ajustados, seleccionando aquellos que proporcionan la mejor precisión en el train set. Posteriormente, llevan a cabo una 4-fold cross validation utilizando toda la base de datos. Por otro lado, como medidas de rendimiento de los clasificadores los autores utilizaron el average accuracy, el Friedman ranking y el Cohen k. Estos concluyen que los mejores clasificadores son los Bosques Aleatorios, seguidos por las Máquinas de Vectores de Soporte. Dentro de los 25 mejores clasificadores, los mejores son los bosques aleatorios implementados en Caret, librería del software R, y que reportan un nivel de precisión del 82.30%, y las máquinas de vectores de soporte con kernel Guassiano, implementadas en el lenguaje C y que reportan un nivel de precisión del 81.8%. En particular, el parallel random forest (parRF t) que ajusta el parámetro mtry, debería ser usado como clasificador de referencia. De forma general, 7 bosques aleatorios y 5 máquinas de soporte están incluídos dentro de los mejores 20 clasificadores. Por otro lado, los mejores resultados se alcanzan con la librería Caret de R. El documento es controversial en el sentido de que ha suscitado diferentes reacciones al respecto. Por un lado, están aquellos que dicen que el documento hace un buen trabajo al cuantificar el impacto de ajustar los parámetros, y que los resultados son consistentes con aquellos encontrados en numerosas competiciones en Kaggle. Pero, por otro lado, están aquellos que sugieren que el documento no muestra más de lo que ya toda la comunidad académica conoce. También, se le podría criticar al documento que, el criterio utilizado para seleccionar el training y el test set podría generar sesgos y altos niveles de varianza, ya que no se cuenta con una partición de los datos estándar, lo cual puede hacer las comparaciones entre experimentos imposible. Sin embargo, el criterio utilizado es eficiente en términos computacionales. También, que debido a que la complejidad de las bases de datos es desconocida, no es posible determinar si el error de clasificación se debe al diseño del clasificador o por dificultades intrínsecas del problema. Sin embargo, dentro de las cosas destacables del documento, se encuentran que compara clasificadores de diferentes familias, lo cual no es usual en este tipo de documentos. El documento utiliza un gran número de bases de datos lo cual hace que los resultados obtenidos se puedan extrapolar a otras bases de datos, cosa que usualmente no ocurre cuando se mide el rendimiento de un modelo en un número pequeño de bases de datos. Una última sugerencia sería presentar los resultados del documento de una forma más amigable e intentar implementar en el análisis otros modelos que están cogiendo cada vez más fuerza y uso (como aquellos desarrollos recientes en Deep Learning). Boosting builds models from individual so called “weak learners” in an iterative way. In boosting, the individual models are not built on completely random subsets of data and features but sequentially by putting more weight on instances with wrong predictions and high errors. The general idea behind this is that instances, which are hard to predict correctly (“difficult” cases) will be focused on during learning, so that the model learns from past mistakes. When we train each ensemble on a subset of the training set, we also call this Stochastic Gradient Boosting, which can help improve generalizability of our model.The gradient is used to minimize a loss function, similar to how Neural Nets utilize gradient descent to optimize (“learn”) weights. In each round of training, the weak learner is built and its predictions are compared to the correct outcome that we expect. The distance between prediction and truth represents the error rate of our model. These errors can now be used to calculate the gradient. The gradient is nothing fancy, it is basically the partial derivative of our loss function - so it describes the steepness of our error function. The gradient can be used to find the direction in which to change the model parameters in order to (maximally) reduce the error in the next round of training by “descending the gradient”.Both xgboost and gbm follows the principle of gradient boosting. There are however, the difference in modeling details. Specifically, xgboost used a more regularized model formalization to control over-fitting, which gives it better performance. The name xgboost, though, actually refers to the engineering goal to push the limit of computations resources for boosted tree algorithms. Which is the reason why many people use xgboost. For model, it might be more suitable to be called as regularized gradient boosting. In addition, the developers of xgboost have made a number of important performance enhancements to different parts of the implementation which make a big difference in speed and memory utilization:1.Use of sparse matrices with sparsity aware algorithms2. Improved data structures for better processor cache utilization which makes it faster.3. Better support for multicore processing which reduces overall training time.In other words, XGBoost is designed for scale in as its initial goal.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(aux).iloc[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Aunque los dos perfiles cuentan con habilidades que son comunes existen diferencias significativas entre el Data Scientist y el Data Engineer. El primero cuenta con una fuerte formación  en matemáticas y estadística para la construcción de modelos predictivos y prescriptivos mientras que el segundo se destaca por sus habilidades en programación, administración de grandes volúmenes de datos y sistemas distribuidos de información. Al hablar de responsabilidades el Data Engineer tiene que encargarse de desarrollar, construir y mantener arquitecturas para la adquisición preparación y distribución de los datos garantizando la confiabilidad, eficiencia y calidad de la información e ideando siempre nuevas y mejores formas para automatizar estas tareas utilizando la herramienta adecuada para cada actividad. En ese sentido el rol del Data Engineer es de servir de apoyo a todos los posibles clientes de esas fuentes de datos incluyendo a los Data Scientist. Por otro lado el Data Scientist se encarga de elaborar modelos y analizar la información para proporcionar al negocio información valiosa y hallazgos presentados de forma comprensible a todas las partes involucradas en la toma de decisiones. Aunque la programación no es la habilidad que caracteriza a este rol si debe hechas mano de lenguajes de programación especializado para poder abordar tareas de obtención de datos, limpieza y análisis. Tiene una fuerte habilidad en la elaboración de informes gráficos que permitan comunicar sus hallazgos. Aunque el conceso no es total en el tema de manera general los tipos de Machine Learning son 4 dependiendo de su enfoque y el propósito de uso: Aprendizaje Superviado (Supervised Learning) Este algoritmo resulta ser el más fácil de entender y aplicar y es el más popular de todos. En este algoritmo el resultado de una función se deduce a partir de datos históricos etiquetados que sirven de entrenamiento. Con el tiempo el algoritmo puede asociar los datos de entrada a una respuesta esperada que ha sido aprendida previamente. Cuando ya se ha completado el entrenamiento, el algoritmo es capaz de predecir resultados a partir de datos de entrada  completamente nuevos. Dentro de este grupo de algoritmos se encuentran los siguientes: \\xa0 Vecinos cercanos. Bayes.  Arboles de decisión. Regresión Lineal.  Support Vector Machine.  Redes Neuronales. Las aplicaciones más destacadas de este algoritmo está el reconocimiento de imágenes, publicidad sugerida, clasificación de correo no deseado. Aprendizaje no supervisado. El algoritmo se encarga de etiquetar los datos de forma autónoma encontrando patrones dentro del conjunto de datos. Es un algoritmo útil para encontrar y segmentar la información que naturalmente no se encuentra etiquetada, escenario muy común en Big Data. Este tipo de algoritmo es altamente aplicado en marketing donde se hace una segmentación del mercado basado en múltiples datos de consumo. Aprendizaje semi supervisado Es una combinación de los algoritmos previos. Aprendizaje Reforzado. En este algoritmo el sistema simula un aprendizaje a base de premios y castigos dentro de un ambiente controlado y orientado por un agente. Al comienzo el algoritmo cometerá muchos errores pero dependiedo del estímulo y la predicción se ajustará la respuesta  En Machine Learing el propósito de los métodos de ensamblaje es combinar diferentes árboles de decisión para mejorar las métricas de predicción que se puedan obtener al usar un único árbol de decisión. El principio detrás de este método es que la suma de diferentes predictores débiles al combinarse o ensamblarse producirán una predicción con mayor precisión.Dentro de las técnicas para realizar el ensamblaje se encuentran:Bagging: La idea principal de es poder generar un nuevo subconjunto de datos a partir de la muestra para training escogiendo con reemplazo y de manera aleatoria los registros de ese subset. A partir de cada colección de datos de la muestra se entrenará cada árbol de decisión. Como resultado se obtiene un ensamblaje de modelos entrenados con un subset de datos de entrenamiento que pueden contener diferentes versiones del set de entrenamiento. El promedio de las predicciones de los diferentes árboles resultará en un modelo más robusto que la predicción de un solo árbol de decisión.Random Forest es una extensión de Baggin donde se agrega un paso extra, donde además de seleccionar de manera aleatoria el subset de datos de entrenamiento se toma de forma aleatoria una selección de las variables o features en lugar de usar todas las variables del set original. El resultado es múltiples árboles entrenados con subset de datos aleatorios y diferentes combinaciones de variables. La ventaja de este algoritmo es que tiene buen desempeño ante altos volúmenes de datos y la precisión no se ve afectada ante datasets que contengan muchos datos faltantes.Boosting es otra técnica de ensamblaje para crear un conjunto de predictores que reduzca el sesgo y la varianza . Existe Bosting para catorias binarias y boosting para categorización multiclase El paper publicado en 2014 hace una evaluación de 179 algoritmos de clasificación de 17 familias de clasificadores incluyendo SVM, random forest, árboles de decisión, redes neuronales, boosting, modelos lineales, regresión multinomial y logística, componentes principales, todos ellos implementados en diferentes plataformas y lenguajes incluyendo R, Weka, C, C++ y Matlab. La evaluación se realiza sobre una colección 121 datasets en su mayoría del UCI Machine Learning Repository más unos datasets de problemas de la vida real. El uso de una colección de datos variada buscó eliminar los sesgos en los resultados del clasificador sobre un tipo de datos en particular, adicionalmente se usó la misma partición Train y Test sobre cada clasificador. De este ejercicio el autor concluye que el mejor clasificador resulta ser el parallel random forest (parRF_t) implementado en R con caret al ajustar el parámetro mtry. Este algoritmo consigue en promedio 94.1% de la precisión máxima lograda sobre todos los datasets (PAMA por sus siglas en inglés). En segundo lugar se encuentra el random forest en R ajustado con caret, que tiene un rendimiento ligeramente inferior con 93.6%. En tercer lugar se encuentra la implementación LibSVM de Support Vector Machine en C ajustando el spread del kernel. Este logra un 92.3% de la máxima precisión. El autor concluye que estas son las mejores familias de clasificadores al tener seis random forest y cinco SVM dentro del top 20 de los mejores clasificadores. Al final considera que el parRF_t debería proponerse como un clasificador Golden estándar sobre el cual deberían basarse las comparaciones de rendimiento de nuevos clasificadores propuestos Gradient Boosting ClassifierGradient Boosting es un algoritmo de Machine Learning que busca crear un estimador robusto a partir de estimadores débiles (generlamente decision trees) desde un enfoque de optimización donde se toma la función de pérdida y se optimiza iterativamente el coste en función del error de la estimación. El concepto fue inicialmente desarrollado por Leo Breiman quien hizo un gran aporte a los algoritmos de árboles de decisión y regresión. En Gradient Boosting se inicia el proceso con un estimador débil y se incorpora en cada iteración otro estimador débil que debe mejorar el desempeño del estimador original al reducir la pérdida de la función de pérdida. La pérdida representa el error residual (la diferencia entre el valor real y la predicción) y se usa esta pérdida para actualizar las predicciones incorporando nuevos estimadores débiles y concentrándolos en las áreas donde los estimadores existentes tuvieron un mal desempeño. Este proceso se repite tantas veces hasta que el error tienda a cero.XGBoost (Extreme Gradient Boosting)XGBoost es una implementación de Gradient Boosting con mejor desempeño y velocidad que ha tenido mucho éxito en las competencias de Machine Learning con datos tabulares o estructurados. La librería incorpora parámetros de optimización que incluyen variables tanto para el modelo como para el sistema en el cual se corre lo que permite sacar el mayor provecho de los recursos de procesamiento y memoria al momento de entrenar y ejecutar el modelo. Dentro de los parámetros para la ejecución del XGBoost está el número de núcleos de CPU que se pueden usar, implementación en ambientes de computación distribuida (Hadoop),\\xa0Out-of-Core Computing\\xa0para conjuntos de datos muy grandes y optimización de memoria caché.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(aux).iloc[1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the `textdistance` for this two examples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10992"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textdistance.hamming(pd.DataFrame(aux).iloc[0][0], pd.DataFrame(aux).iloc[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a matrix for these similarities!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>10992</td>\n",
       "      <td>10800</td>\n",
       "      <td>10924</td>\n",
       "      <td>10898</td>\n",
       "      <td>10987</td>\n",
       "      <td>11002</td>\n",
       "      <td>11953</td>\n",
       "      <td>11001</td>\n",
       "      <td>10968</td>\n",
       "      <td>...</td>\n",
       "      <td>11116</td>\n",
       "      <td>10928</td>\n",
       "      <td>10714</td>\n",
       "      <td>10932</td>\n",
       "      <td>12106</td>\n",
       "      <td>13642</td>\n",
       "      <td>11482</td>\n",
       "      <td>12482</td>\n",
       "      <td>10884</td>\n",
       "      <td>10984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10992</td>\n",
       "      <td>0</td>\n",
       "      <td>10488</td>\n",
       "      <td>9002</td>\n",
       "      <td>9626</td>\n",
       "      <td>7965</td>\n",
       "      <td>7974</td>\n",
       "      <td>12128</td>\n",
       "      <td>7945</td>\n",
       "      <td>11113</td>\n",
       "      <td>...</td>\n",
       "      <td>8011</td>\n",
       "      <td>11117</td>\n",
       "      <td>10948</td>\n",
       "      <td>8635</td>\n",
       "      <td>12337</td>\n",
       "      <td>13878</td>\n",
       "      <td>8402</td>\n",
       "      <td>12667</td>\n",
       "      <td>8711</td>\n",
       "      <td>7922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10800</td>\n",
       "      <td>10488</td>\n",
       "      <td>0</td>\n",
       "      <td>10392</td>\n",
       "      <td>10317</td>\n",
       "      <td>10540</td>\n",
       "      <td>10573</td>\n",
       "      <td>11917</td>\n",
       "      <td>10524</td>\n",
       "      <td>10998</td>\n",
       "      <td>...</td>\n",
       "      <td>10581</td>\n",
       "      <td>10992</td>\n",
       "      <td>10800</td>\n",
       "      <td>10455</td>\n",
       "      <td>12121</td>\n",
       "      <td>13658</td>\n",
       "      <td>11014</td>\n",
       "      <td>12491</td>\n",
       "      <td>10447</td>\n",
       "      <td>10487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10924</td>\n",
       "      <td>9002</td>\n",
       "      <td>10392</td>\n",
       "      <td>0</td>\n",
       "      <td>9546</td>\n",
       "      <td>9033</td>\n",
       "      <td>9024</td>\n",
       "      <td>12065</td>\n",
       "      <td>8960</td>\n",
       "      <td>11087</td>\n",
       "      <td>...</td>\n",
       "      <td>9114</td>\n",
       "      <td>11019</td>\n",
       "      <td>10873</td>\n",
       "      <td>8939</td>\n",
       "      <td>12232</td>\n",
       "      <td>13770</td>\n",
       "      <td>9483</td>\n",
       "      <td>12573</td>\n",
       "      <td>8938</td>\n",
       "      <td>8993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10898</td>\n",
       "      <td>9626</td>\n",
       "      <td>10317</td>\n",
       "      <td>9546</td>\n",
       "      <td>0</td>\n",
       "      <td>9659</td>\n",
       "      <td>9628</td>\n",
       "      <td>11951</td>\n",
       "      <td>9667</td>\n",
       "      <td>11044</td>\n",
       "      <td>...</td>\n",
       "      <td>9739</td>\n",
       "      <td>11010</td>\n",
       "      <td>10833</td>\n",
       "      <td>9608</td>\n",
       "      <td>12185</td>\n",
       "      <td>13718</td>\n",
       "      <td>10132</td>\n",
       "      <td>12590</td>\n",
       "      <td>9512</td>\n",
       "      <td>9634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10987</td>\n",
       "      <td>7965</td>\n",
       "      <td>10540</td>\n",
       "      <td>9033</td>\n",
       "      <td>9659</td>\n",
       "      <td>0</td>\n",
       "      <td>7548</td>\n",
       "      <td>12207</td>\n",
       "      <td>7883</td>\n",
       "      <td>11206</td>\n",
       "      <td>...</td>\n",
       "      <td>7418</td>\n",
       "      <td>11178</td>\n",
       "      <td>10991</td>\n",
       "      <td>8681</td>\n",
       "      <td>12357</td>\n",
       "      <td>13915</td>\n",
       "      <td>7805</td>\n",
       "      <td>12776</td>\n",
       "      <td>8815</td>\n",
       "      <td>8016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>11002</td>\n",
       "      <td>7974</td>\n",
       "      <td>10573</td>\n",
       "      <td>9024</td>\n",
       "      <td>9628</td>\n",
       "      <td>7548</td>\n",
       "      <td>0</td>\n",
       "      <td>12138</td>\n",
       "      <td>7882</td>\n",
       "      <td>11204</td>\n",
       "      <td>...</td>\n",
       "      <td>7595</td>\n",
       "      <td>11205</td>\n",
       "      <td>10982</td>\n",
       "      <td>8647</td>\n",
       "      <td>12377</td>\n",
       "      <td>13894</td>\n",
       "      <td>8010</td>\n",
       "      <td>12795</td>\n",
       "      <td>8814</td>\n",
       "      <td>8033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>11953</td>\n",
       "      <td>12128</td>\n",
       "      <td>11917</td>\n",
       "      <td>12065</td>\n",
       "      <td>11951</td>\n",
       "      <td>12207</td>\n",
       "      <td>12138</td>\n",
       "      <td>0</td>\n",
       "      <td>12166</td>\n",
       "      <td>11960</td>\n",
       "      <td>...</td>\n",
       "      <td>12275</td>\n",
       "      <td>11903</td>\n",
       "      <td>11926</td>\n",
       "      <td>12058</td>\n",
       "      <td>12054</td>\n",
       "      <td>13516</td>\n",
       "      <td>12663</td>\n",
       "      <td>12382</td>\n",
       "      <td>12096</td>\n",
       "      <td>12188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>11001</td>\n",
       "      <td>7945</td>\n",
       "      <td>10524</td>\n",
       "      <td>8960</td>\n",
       "      <td>9667</td>\n",
       "      <td>7883</td>\n",
       "      <td>7882</td>\n",
       "      <td>12166</td>\n",
       "      <td>0</td>\n",
       "      <td>11176</td>\n",
       "      <td>...</td>\n",
       "      <td>7929</td>\n",
       "      <td>11148</td>\n",
       "      <td>10922</td>\n",
       "      <td>8665</td>\n",
       "      <td>12308</td>\n",
       "      <td>13837</td>\n",
       "      <td>8361</td>\n",
       "      <td>12764</td>\n",
       "      <td>8786</td>\n",
       "      <td>7963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10968</td>\n",
       "      <td>11113</td>\n",
       "      <td>10998</td>\n",
       "      <td>11087</td>\n",
       "      <td>11044</td>\n",
       "      <td>11206</td>\n",
       "      <td>11204</td>\n",
       "      <td>11960</td>\n",
       "      <td>11176</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>11241</td>\n",
       "      <td>10910</td>\n",
       "      <td>10937</td>\n",
       "      <td>11077</td>\n",
       "      <td>12031</td>\n",
       "      <td>13528</td>\n",
       "      <td>11674</td>\n",
       "      <td>12433</td>\n",
       "      <td>11053</td>\n",
       "      <td>11093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10885</td>\n",
       "      <td>8972</td>\n",
       "      <td>10402</td>\n",
       "      <td>8932</td>\n",
       "      <td>9481</td>\n",
       "      <td>8979</td>\n",
       "      <td>8979</td>\n",
       "      <td>12041</td>\n",
       "      <td>8973</td>\n",
       "      <td>11027</td>\n",
       "      <td>...</td>\n",
       "      <td>9049</td>\n",
       "      <td>11069</td>\n",
       "      <td>10841</td>\n",
       "      <td>8948</td>\n",
       "      <td>12234</td>\n",
       "      <td>13689</td>\n",
       "      <td>9495</td>\n",
       "      <td>12604</td>\n",
       "      <td>8908</td>\n",
       "      <td>8966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>13239</td>\n",
       "      <td>13407</td>\n",
       "      <td>13172</td>\n",
       "      <td>13292</td>\n",
       "      <td>13266</td>\n",
       "      <td>13514</td>\n",
       "      <td>13421</td>\n",
       "      <td>13101</td>\n",
       "      <td>13409</td>\n",
       "      <td>13189</td>\n",
       "      <td>...</td>\n",
       "      <td>13487</td>\n",
       "      <td>13125</td>\n",
       "      <td>13237</td>\n",
       "      <td>13370</td>\n",
       "      <td>13044</td>\n",
       "      <td>13525</td>\n",
       "      <td>13930</td>\n",
       "      <td>13132</td>\n",
       "      <td>13317</td>\n",
       "      <td>13349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10790</td>\n",
       "      <td>10553</td>\n",
       "      <td>10365</td>\n",
       "      <td>10440</td>\n",
       "      <td>10412</td>\n",
       "      <td>10626</td>\n",
       "      <td>10605</td>\n",
       "      <td>11984</td>\n",
       "      <td>10579</td>\n",
       "      <td>10901</td>\n",
       "      <td>...</td>\n",
       "      <td>10672</td>\n",
       "      <td>10989</td>\n",
       "      <td>10756</td>\n",
       "      <td>10505</td>\n",
       "      <td>12129</td>\n",
       "      <td>13670</td>\n",
       "      <td>11055</td>\n",
       "      <td>12493</td>\n",
       "      <td>10455</td>\n",
       "      <td>10503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10793</td>\n",
       "      <td>9693</td>\n",
       "      <td>10429</td>\n",
       "      <td>9610</td>\n",
       "      <td>9549</td>\n",
       "      <td>9774</td>\n",
       "      <td>9751</td>\n",
       "      <td>11963</td>\n",
       "      <td>9635</td>\n",
       "      <td>10992</td>\n",
       "      <td>...</td>\n",
       "      <td>9780</td>\n",
       "      <td>11020</td>\n",
       "      <td>10872</td>\n",
       "      <td>9648</td>\n",
       "      <td>12113</td>\n",
       "      <td>13716</td>\n",
       "      <td>10187</td>\n",
       "      <td>12568</td>\n",
       "      <td>9640</td>\n",
       "      <td>9644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>11317</td>\n",
       "      <td>8273</td>\n",
       "      <td>10864</td>\n",
       "      <td>9283</td>\n",
       "      <td>9983</td>\n",
       "      <td>7657</td>\n",
       "      <td>7862</td>\n",
       "      <td>12508</td>\n",
       "      <td>8202</td>\n",
       "      <td>11517</td>\n",
       "      <td>...</td>\n",
       "      <td>6826</td>\n",
       "      <td>11502</td>\n",
       "      <td>11278</td>\n",
       "      <td>8980</td>\n",
       "      <td>12679</td>\n",
       "      <td>14215</td>\n",
       "      <td>3415</td>\n",
       "      <td>13087</td>\n",
       "      <td>9121</td>\n",
       "      <td>8305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10992</td>\n",
       "      <td>8215</td>\n",
       "      <td>10467</td>\n",
       "      <td>8956</td>\n",
       "      <td>9591</td>\n",
       "      <td>8280</td>\n",
       "      <td>8294</td>\n",
       "      <td>12173</td>\n",
       "      <td>8205</td>\n",
       "      <td>11136</td>\n",
       "      <td>...</td>\n",
       "      <td>8332</td>\n",
       "      <td>11120</td>\n",
       "      <td>10913</td>\n",
       "      <td>8638</td>\n",
       "      <td>12288</td>\n",
       "      <td>13849</td>\n",
       "      <td>8723</td>\n",
       "      <td>12679</td>\n",
       "      <td>8798</td>\n",
       "      <td>8255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10930</td>\n",
       "      <td>8351</td>\n",
       "      <td>10491</td>\n",
       "      <td>8958</td>\n",
       "      <td>9608</td>\n",
       "      <td>8415</td>\n",
       "      <td>8409</td>\n",
       "      <td>12113</td>\n",
       "      <td>8366</td>\n",
       "      <td>11127</td>\n",
       "      <td>...</td>\n",
       "      <td>8452</td>\n",
       "      <td>11061</td>\n",
       "      <td>10913</td>\n",
       "      <td>8629</td>\n",
       "      <td>12283</td>\n",
       "      <td>13828</td>\n",
       "      <td>8842</td>\n",
       "      <td>12711</td>\n",
       "      <td>8754</td>\n",
       "      <td>8360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>15098</td>\n",
       "      <td>15295</td>\n",
       "      <td>15080</td>\n",
       "      <td>15219</td>\n",
       "      <td>15107</td>\n",
       "      <td>15348</td>\n",
       "      <td>15313</td>\n",
       "      <td>15066</td>\n",
       "      <td>15320</td>\n",
       "      <td>15008</td>\n",
       "      <td>...</td>\n",
       "      <td>15444</td>\n",
       "      <td>15074</td>\n",
       "      <td>15098</td>\n",
       "      <td>15272</td>\n",
       "      <td>15066</td>\n",
       "      <td>14884</td>\n",
       "      <td>15805</td>\n",
       "      <td>14925</td>\n",
       "      <td>15186</td>\n",
       "      <td>15306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>11051</td>\n",
       "      <td>7945</td>\n",
       "      <td>10532</td>\n",
       "      <td>8983</td>\n",
       "      <td>9720</td>\n",
       "      <td>7364</td>\n",
       "      <td>7523</td>\n",
       "      <td>12211</td>\n",
       "      <td>7913</td>\n",
       "      <td>11189</td>\n",
       "      <td>...</td>\n",
       "      <td>7289</td>\n",
       "      <td>11138</td>\n",
       "      <td>11016</td>\n",
       "      <td>8671</td>\n",
       "      <td>12398</td>\n",
       "      <td>13856</td>\n",
       "      <td>7721</td>\n",
       "      <td>12711</td>\n",
       "      <td>8811</td>\n",
       "      <td>8006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>11104</td>\n",
       "      <td>8014</td>\n",
       "      <td>10615</td>\n",
       "      <td>9102</td>\n",
       "      <td>9756</td>\n",
       "      <td>7419</td>\n",
       "      <td>7650</td>\n",
       "      <td>12294</td>\n",
       "      <td>8011</td>\n",
       "      <td>11282</td>\n",
       "      <td>...</td>\n",
       "      <td>6533</td>\n",
       "      <td>11292</td>\n",
       "      <td>11055</td>\n",
       "      <td>8728</td>\n",
       "      <td>12440</td>\n",
       "      <td>13998</td>\n",
       "      <td>6654</td>\n",
       "      <td>12800</td>\n",
       "      <td>8873</td>\n",
       "      <td>8069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>11042</td>\n",
       "      <td>8014</td>\n",
       "      <td>10574</td>\n",
       "      <td>9073</td>\n",
       "      <td>9701</td>\n",
       "      <td>7382</td>\n",
       "      <td>7600</td>\n",
       "      <td>12242</td>\n",
       "      <td>7970</td>\n",
       "      <td>11231</td>\n",
       "      <td>...</td>\n",
       "      <td>6910</td>\n",
       "      <td>11175</td>\n",
       "      <td>11015</td>\n",
       "      <td>8733</td>\n",
       "      <td>12395</td>\n",
       "      <td>13964</td>\n",
       "      <td>7333</td>\n",
       "      <td>12795</td>\n",
       "      <td>8874</td>\n",
       "      <td>8058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21236</td>\n",
       "      <td>21506</td>\n",
       "      <td>21226</td>\n",
       "      <td>21349</td>\n",
       "      <td>21337</td>\n",
       "      <td>21525</td>\n",
       "      <td>21493</td>\n",
       "      <td>21173</td>\n",
       "      <td>21478</td>\n",
       "      <td>21201</td>\n",
       "      <td>...</td>\n",
       "      <td>21574</td>\n",
       "      <td>21274</td>\n",
       "      <td>21277</td>\n",
       "      <td>21453</td>\n",
       "      <td>21140</td>\n",
       "      <td>20987</td>\n",
       "      <td>21980</td>\n",
       "      <td>21123</td>\n",
       "      <td>21417</td>\n",
       "      <td>21475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>11261</td>\n",
       "      <td>8231</td>\n",
       "      <td>10815</td>\n",
       "      <td>9333</td>\n",
       "      <td>9932</td>\n",
       "      <td>7577</td>\n",
       "      <td>7814</td>\n",
       "      <td>12464</td>\n",
       "      <td>8167</td>\n",
       "      <td>11464</td>\n",
       "      <td>...</td>\n",
       "      <td>6784</td>\n",
       "      <td>11452</td>\n",
       "      <td>11214</td>\n",
       "      <td>8916</td>\n",
       "      <td>12665</td>\n",
       "      <td>14123</td>\n",
       "      <td>4083</td>\n",
       "      <td>13046</td>\n",
       "      <td>9075</td>\n",
       "      <td>8259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>10981</td>\n",
       "      <td>7973</td>\n",
       "      <td>10535</td>\n",
       "      <td>9024</td>\n",
       "      <td>9647</td>\n",
       "      <td>7661</td>\n",
       "      <td>7684</td>\n",
       "      <td>12153</td>\n",
       "      <td>7907</td>\n",
       "      <td>11216</td>\n",
       "      <td>...</td>\n",
       "      <td>7718</td>\n",
       "      <td>11138</td>\n",
       "      <td>10901</td>\n",
       "      <td>8656</td>\n",
       "      <td>12331</td>\n",
       "      <td>13888</td>\n",
       "      <td>8118</td>\n",
       "      <td>12736</td>\n",
       "      <td>8771</td>\n",
       "      <td>8020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>10884</td>\n",
       "      <td>8553</td>\n",
       "      <td>10450</td>\n",
       "      <td>8895</td>\n",
       "      <td>9595</td>\n",
       "      <td>8651</td>\n",
       "      <td>8593</td>\n",
       "      <td>12103</td>\n",
       "      <td>8549</td>\n",
       "      <td>11017</td>\n",
       "      <td>...</td>\n",
       "      <td>8658</td>\n",
       "      <td>11144</td>\n",
       "      <td>10868</td>\n",
       "      <td>8541</td>\n",
       "      <td>12263</td>\n",
       "      <td>13798</td>\n",
       "      <td>9067</td>\n",
       "      <td>12654</td>\n",
       "      <td>8682</td>\n",
       "      <td>8540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>10731</td>\n",
       "      <td>10788</td>\n",
       "      <td>10686</td>\n",
       "      <td>10787</td>\n",
       "      <td>10726</td>\n",
       "      <td>10879</td>\n",
       "      <td>10914</td>\n",
       "      <td>11903</td>\n",
       "      <td>10840</td>\n",
       "      <td>10898</td>\n",
       "      <td>...</td>\n",
       "      <td>10970</td>\n",
       "      <td>10936</td>\n",
       "      <td>10698</td>\n",
       "      <td>10795</td>\n",
       "      <td>12115</td>\n",
       "      <td>13618</td>\n",
       "      <td>11381</td>\n",
       "      <td>12441</td>\n",
       "      <td>10714</td>\n",
       "      <td>10823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>12604</td>\n",
       "      <td>12925</td>\n",
       "      <td>12738</td>\n",
       "      <td>12769</td>\n",
       "      <td>12790</td>\n",
       "      <td>12954</td>\n",
       "      <td>12937</td>\n",
       "      <td>12635</td>\n",
       "      <td>12896</td>\n",
       "      <td>12680</td>\n",
       "      <td>...</td>\n",
       "      <td>13000</td>\n",
       "      <td>12646</td>\n",
       "      <td>12698</td>\n",
       "      <td>12865</td>\n",
       "      <td>12586</td>\n",
       "      <td>13495</td>\n",
       "      <td>13397</td>\n",
       "      <td>12475</td>\n",
       "      <td>12856</td>\n",
       "      <td>12846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>11148</td>\n",
       "      <td>8130</td>\n",
       "      <td>10715</td>\n",
       "      <td>9148</td>\n",
       "      <td>9830</td>\n",
       "      <td>7536</td>\n",
       "      <td>7701</td>\n",
       "      <td>12354</td>\n",
       "      <td>8063</td>\n",
       "      <td>11314</td>\n",
       "      <td>...</td>\n",
       "      <td>6700</td>\n",
       "      <td>11341</td>\n",
       "      <td>11130</td>\n",
       "      <td>8853</td>\n",
       "      <td>12536</td>\n",
       "      <td>14078</td>\n",
       "      <td>5793</td>\n",
       "      <td>12879</td>\n",
       "      <td>8951</td>\n",
       "      <td>8182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>11121</td>\n",
       "      <td>8010</td>\n",
       "      <td>10638</td>\n",
       "      <td>9114</td>\n",
       "      <td>9779</td>\n",
       "      <td>7464</td>\n",
       "      <td>7619</td>\n",
       "      <td>12298</td>\n",
       "      <td>7981</td>\n",
       "      <td>11267</td>\n",
       "      <td>...</td>\n",
       "      <td>6589</td>\n",
       "      <td>11275</td>\n",
       "      <td>11074</td>\n",
       "      <td>8740</td>\n",
       "      <td>12462</td>\n",
       "      <td>13949</td>\n",
       "      <td>6596</td>\n",
       "      <td>12820</td>\n",
       "      <td>8912</td>\n",
       "      <td>8092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>10950</td>\n",
       "      <td>11226</td>\n",
       "      <td>11063</td>\n",
       "      <td>11124</td>\n",
       "      <td>11097</td>\n",
       "      <td>11330</td>\n",
       "      <td>11300</td>\n",
       "      <td>11899</td>\n",
       "      <td>11237</td>\n",
       "      <td>11001</td>\n",
       "      <td>...</td>\n",
       "      <td>11333</td>\n",
       "      <td>10992</td>\n",
       "      <td>10994</td>\n",
       "      <td>11205</td>\n",
       "      <td>12092</td>\n",
       "      <td>13630</td>\n",
       "      <td>11753</td>\n",
       "      <td>12450</td>\n",
       "      <td>11143</td>\n",
       "      <td>11204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>11009</td>\n",
       "      <td>7917</td>\n",
       "      <td>10474</td>\n",
       "      <td>8905</td>\n",
       "      <td>9618</td>\n",
       "      <td>7970</td>\n",
       "      <td>7974</td>\n",
       "      <td>12115</td>\n",
       "      <td>7949</td>\n",
       "      <td>11147</td>\n",
       "      <td>...</td>\n",
       "      <td>8033</td>\n",
       "      <td>11143</td>\n",
       "      <td>10909</td>\n",
       "      <td>8609</td>\n",
       "      <td>12325</td>\n",
       "      <td>13854</td>\n",
       "      <td>8461</td>\n",
       "      <td>12668</td>\n",
       "      <td>8721</td>\n",
       "      <td>7929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>10902</td>\n",
       "      <td>8259</td>\n",
       "      <td>10450</td>\n",
       "      <td>8969</td>\n",
       "      <td>9586</td>\n",
       "      <td>8325</td>\n",
       "      <td>8318</td>\n",
       "      <td>12069</td>\n",
       "      <td>8308</td>\n",
       "      <td>11124</td>\n",
       "      <td>...</td>\n",
       "      <td>8403</td>\n",
       "      <td>11116</td>\n",
       "      <td>10895</td>\n",
       "      <td>8607</td>\n",
       "      <td>12329</td>\n",
       "      <td>13813</td>\n",
       "      <td>8791</td>\n",
       "      <td>12689</td>\n",
       "      <td>8742</td>\n",
       "      <td>8334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>11116</td>\n",
       "      <td>8011</td>\n",
       "      <td>10581</td>\n",
       "      <td>9114</td>\n",
       "      <td>9739</td>\n",
       "      <td>7418</td>\n",
       "      <td>7595</td>\n",
       "      <td>12275</td>\n",
       "      <td>7929</td>\n",
       "      <td>11241</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>11274</td>\n",
       "      <td>11090</td>\n",
       "      <td>8688</td>\n",
       "      <td>12432</td>\n",
       "      <td>13961</td>\n",
       "      <td>6978</td>\n",
       "      <td>12823</td>\n",
       "      <td>8800</td>\n",
       "      <td>8090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>10928</td>\n",
       "      <td>11117</td>\n",
       "      <td>10992</td>\n",
       "      <td>11019</td>\n",
       "      <td>11010</td>\n",
       "      <td>11178</td>\n",
       "      <td>11205</td>\n",
       "      <td>11903</td>\n",
       "      <td>11148</td>\n",
       "      <td>10910</td>\n",
       "      <td>...</td>\n",
       "      <td>11274</td>\n",
       "      <td>0</td>\n",
       "      <td>11006</td>\n",
       "      <td>11077</td>\n",
       "      <td>12116</td>\n",
       "      <td>13634</td>\n",
       "      <td>11636</td>\n",
       "      <td>12483</td>\n",
       "      <td>11093</td>\n",
       "      <td>11129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>10714</td>\n",
       "      <td>10948</td>\n",
       "      <td>10800</td>\n",
       "      <td>10873</td>\n",
       "      <td>10833</td>\n",
       "      <td>10991</td>\n",
       "      <td>10982</td>\n",
       "      <td>11926</td>\n",
       "      <td>10922</td>\n",
       "      <td>10937</td>\n",
       "      <td>...</td>\n",
       "      <td>11090</td>\n",
       "      <td>11006</td>\n",
       "      <td>0</td>\n",
       "      <td>10890</td>\n",
       "      <td>12097</td>\n",
       "      <td>13564</td>\n",
       "      <td>11436</td>\n",
       "      <td>12600</td>\n",
       "      <td>10863</td>\n",
       "      <td>10880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>10932</td>\n",
       "      <td>8635</td>\n",
       "      <td>10455</td>\n",
       "      <td>8939</td>\n",
       "      <td>9608</td>\n",
       "      <td>8681</td>\n",
       "      <td>8647</td>\n",
       "      <td>12058</td>\n",
       "      <td>8665</td>\n",
       "      <td>11077</td>\n",
       "      <td>...</td>\n",
       "      <td>8688</td>\n",
       "      <td>11077</td>\n",
       "      <td>10890</td>\n",
       "      <td>0</td>\n",
       "      <td>12337</td>\n",
       "      <td>13765</td>\n",
       "      <td>9125</td>\n",
       "      <td>12682</td>\n",
       "      <td>8701</td>\n",
       "      <td>8655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>12106</td>\n",
       "      <td>12337</td>\n",
       "      <td>12121</td>\n",
       "      <td>12232</td>\n",
       "      <td>12185</td>\n",
       "      <td>12357</td>\n",
       "      <td>12377</td>\n",
       "      <td>12054</td>\n",
       "      <td>12308</td>\n",
       "      <td>12031</td>\n",
       "      <td>...</td>\n",
       "      <td>12432</td>\n",
       "      <td>12116</td>\n",
       "      <td>12097</td>\n",
       "      <td>12337</td>\n",
       "      <td>0</td>\n",
       "      <td>13567</td>\n",
       "      <td>12831</td>\n",
       "      <td>12397</td>\n",
       "      <td>12254</td>\n",
       "      <td>12283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>13642</td>\n",
       "      <td>13878</td>\n",
       "      <td>13658</td>\n",
       "      <td>13770</td>\n",
       "      <td>13718</td>\n",
       "      <td>13915</td>\n",
       "      <td>13894</td>\n",
       "      <td>13516</td>\n",
       "      <td>13837</td>\n",
       "      <td>13528</td>\n",
       "      <td>...</td>\n",
       "      <td>13961</td>\n",
       "      <td>13634</td>\n",
       "      <td>13564</td>\n",
       "      <td>13765</td>\n",
       "      <td>13567</td>\n",
       "      <td>0</td>\n",
       "      <td>14368</td>\n",
       "      <td>13568</td>\n",
       "      <td>13768</td>\n",
       "      <td>13789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>11482</td>\n",
       "      <td>8402</td>\n",
       "      <td>11014</td>\n",
       "      <td>9483</td>\n",
       "      <td>10132</td>\n",
       "      <td>7805</td>\n",
       "      <td>8010</td>\n",
       "      <td>12663</td>\n",
       "      <td>8361</td>\n",
       "      <td>11674</td>\n",
       "      <td>...</td>\n",
       "      <td>6978</td>\n",
       "      <td>11636</td>\n",
       "      <td>11436</td>\n",
       "      <td>9125</td>\n",
       "      <td>12831</td>\n",
       "      <td>14368</td>\n",
       "      <td>0</td>\n",
       "      <td>13225</td>\n",
       "      <td>9266</td>\n",
       "      <td>8479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>12482</td>\n",
       "      <td>12667</td>\n",
       "      <td>12491</td>\n",
       "      <td>12573</td>\n",
       "      <td>12590</td>\n",
       "      <td>12776</td>\n",
       "      <td>12795</td>\n",
       "      <td>12382</td>\n",
       "      <td>12764</td>\n",
       "      <td>12433</td>\n",
       "      <td>...</td>\n",
       "      <td>12823</td>\n",
       "      <td>12483</td>\n",
       "      <td>12600</td>\n",
       "      <td>12682</td>\n",
       "      <td>12397</td>\n",
       "      <td>13568</td>\n",
       "      <td>13225</td>\n",
       "      <td>0</td>\n",
       "      <td>12612</td>\n",
       "      <td>12733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>10884</td>\n",
       "      <td>8711</td>\n",
       "      <td>10447</td>\n",
       "      <td>8938</td>\n",
       "      <td>9512</td>\n",
       "      <td>8815</td>\n",
       "      <td>8814</td>\n",
       "      <td>12096</td>\n",
       "      <td>8786</td>\n",
       "      <td>11053</td>\n",
       "      <td>...</td>\n",
       "      <td>8800</td>\n",
       "      <td>11093</td>\n",
       "      <td>10863</td>\n",
       "      <td>8701</td>\n",
       "      <td>12254</td>\n",
       "      <td>13768</td>\n",
       "      <td>9266</td>\n",
       "      <td>12612</td>\n",
       "      <td>0</td>\n",
       "      <td>8754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>10984</td>\n",
       "      <td>7922</td>\n",
       "      <td>10487</td>\n",
       "      <td>8993</td>\n",
       "      <td>9634</td>\n",
       "      <td>8016</td>\n",
       "      <td>8033</td>\n",
       "      <td>12188</td>\n",
       "      <td>7963</td>\n",
       "      <td>11093</td>\n",
       "      <td>...</td>\n",
       "      <td>8090</td>\n",
       "      <td>11129</td>\n",
       "      <td>10880</td>\n",
       "      <td>8655</td>\n",
       "      <td>12283</td>\n",
       "      <td>13789</td>\n",
       "      <td>8479</td>\n",
       "      <td>12733</td>\n",
       "      <td>8754</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1      2      3      4      5      6      7      8      9   \\\n",
       "0       0  10992  10800  10924  10898  10987  11002  11953  11001  10968   \n",
       "1   10992      0  10488   9002   9626   7965   7974  12128   7945  11113   \n",
       "2   10800  10488      0  10392  10317  10540  10573  11917  10524  10998   \n",
       "3   10924   9002  10392      0   9546   9033   9024  12065   8960  11087   \n",
       "4   10898   9626  10317   9546      0   9659   9628  11951   9667  11044   \n",
       "5   10987   7965  10540   9033   9659      0   7548  12207   7883  11206   \n",
       "6   11002   7974  10573   9024   9628   7548      0  12138   7882  11204   \n",
       "7   11953  12128  11917  12065  11951  12207  12138      0  12166  11960   \n",
       "8   11001   7945  10524   8960   9667   7883   7882  12166      0  11176   \n",
       "9   10968  11113  10998  11087  11044  11206  11204  11960  11176      0   \n",
       "10  10885   8972  10402   8932   9481   8979   8979  12041   8973  11027   \n",
       "11  13239  13407  13172  13292  13266  13514  13421  13101  13409  13189   \n",
       "12  10790  10553  10365  10440  10412  10626  10605  11984  10579  10901   \n",
       "13  10793   9693  10429   9610   9549   9774   9751  11963   9635  10992   \n",
       "14  11317   8273  10864   9283   9983   7657   7862  12508   8202  11517   \n",
       "15  10992   8215  10467   8956   9591   8280   8294  12173   8205  11136   \n",
       "16  10930   8351  10491   8958   9608   8415   8409  12113   8366  11127   \n",
       "17  15098  15295  15080  15219  15107  15348  15313  15066  15320  15008   \n",
       "18  11051   7945  10532   8983   9720   7364   7523  12211   7913  11189   \n",
       "19  11104   8014  10615   9102   9756   7419   7650  12294   8011  11282   \n",
       "20  11042   8014  10574   9073   9701   7382   7600  12242   7970  11231   \n",
       "21  21236  21506  21226  21349  21337  21525  21493  21173  21478  21201   \n",
       "22  11261   8231  10815   9333   9932   7577   7814  12464   8167  11464   \n",
       "23  10981   7973  10535   9024   9647   7661   7684  12153   7907  11216   \n",
       "24  10884   8553  10450   8895   9595   8651   8593  12103   8549  11017   \n",
       "25  10731  10788  10686  10787  10726  10879  10914  11903  10840  10898   \n",
       "26  12604  12925  12738  12769  12790  12954  12937  12635  12896  12680   \n",
       "27  11148   8130  10715   9148   9830   7536   7701  12354   8063  11314   \n",
       "28  11121   8010  10638   9114   9779   7464   7619  12298   7981  11267   \n",
       "29  10950  11226  11063  11124  11097  11330  11300  11899  11237  11001   \n",
       "30  11009   7917  10474   8905   9618   7970   7974  12115   7949  11147   \n",
       "31  10902   8259  10450   8969   9586   8325   8318  12069   8308  11124   \n",
       "32  11116   8011  10581   9114   9739   7418   7595  12275   7929  11241   \n",
       "33  10928  11117  10992  11019  11010  11178  11205  11903  11148  10910   \n",
       "34  10714  10948  10800  10873  10833  10991  10982  11926  10922  10937   \n",
       "35  10932   8635  10455   8939   9608   8681   8647  12058   8665  11077   \n",
       "36  12106  12337  12121  12232  12185  12357  12377  12054  12308  12031   \n",
       "37  13642  13878  13658  13770  13718  13915  13894  13516  13837  13528   \n",
       "38  11482   8402  11014   9483  10132   7805   8010  12663   8361  11674   \n",
       "39  12482  12667  12491  12573  12590  12776  12795  12382  12764  12433   \n",
       "40  10884   8711  10447   8938   9512   8815   8814  12096   8786  11053   \n",
       "41  10984   7922  10487   8993   9634   8016   8033  12188   7963  11093   \n",
       "\n",
       "    ...       32     33     34     35     36     37     38     39     40  \\\n",
       "0   ...    11116  10928  10714  10932  12106  13642  11482  12482  10884   \n",
       "1   ...     8011  11117  10948   8635  12337  13878   8402  12667   8711   \n",
       "2   ...    10581  10992  10800  10455  12121  13658  11014  12491  10447   \n",
       "3   ...     9114  11019  10873   8939  12232  13770   9483  12573   8938   \n",
       "4   ...     9739  11010  10833   9608  12185  13718  10132  12590   9512   \n",
       "5   ...     7418  11178  10991   8681  12357  13915   7805  12776   8815   \n",
       "6   ...     7595  11205  10982   8647  12377  13894   8010  12795   8814   \n",
       "7   ...    12275  11903  11926  12058  12054  13516  12663  12382  12096   \n",
       "8   ...     7929  11148  10922   8665  12308  13837   8361  12764   8786   \n",
       "9   ...    11241  10910  10937  11077  12031  13528  11674  12433  11053   \n",
       "10  ...     9049  11069  10841   8948  12234  13689   9495  12604   8908   \n",
       "11  ...    13487  13125  13237  13370  13044  13525  13930  13132  13317   \n",
       "12  ...    10672  10989  10756  10505  12129  13670  11055  12493  10455   \n",
       "13  ...     9780  11020  10872   9648  12113  13716  10187  12568   9640   \n",
       "14  ...     6826  11502  11278   8980  12679  14215   3415  13087   9121   \n",
       "15  ...     8332  11120  10913   8638  12288  13849   8723  12679   8798   \n",
       "16  ...     8452  11061  10913   8629  12283  13828   8842  12711   8754   \n",
       "17  ...    15444  15074  15098  15272  15066  14884  15805  14925  15186   \n",
       "18  ...     7289  11138  11016   8671  12398  13856   7721  12711   8811   \n",
       "19  ...     6533  11292  11055   8728  12440  13998   6654  12800   8873   \n",
       "20  ...     6910  11175  11015   8733  12395  13964   7333  12795   8874   \n",
       "21  ...    21574  21274  21277  21453  21140  20987  21980  21123  21417   \n",
       "22  ...     6784  11452  11214   8916  12665  14123   4083  13046   9075   \n",
       "23  ...     7718  11138  10901   8656  12331  13888   8118  12736   8771   \n",
       "24  ...     8658  11144  10868   8541  12263  13798   9067  12654   8682   \n",
       "25  ...    10970  10936  10698  10795  12115  13618  11381  12441  10714   \n",
       "26  ...    13000  12646  12698  12865  12586  13495  13397  12475  12856   \n",
       "27  ...     6700  11341  11130   8853  12536  14078   5793  12879   8951   \n",
       "28  ...     6589  11275  11074   8740  12462  13949   6596  12820   8912   \n",
       "29  ...    11333  10992  10994  11205  12092  13630  11753  12450  11143   \n",
       "30  ...     8033  11143  10909   8609  12325  13854   8461  12668   8721   \n",
       "31  ...     8403  11116  10895   8607  12329  13813   8791  12689   8742   \n",
       "32  ...        0  11274  11090   8688  12432  13961   6978  12823   8800   \n",
       "33  ...    11274      0  11006  11077  12116  13634  11636  12483  11093   \n",
       "34  ...    11090  11006      0  10890  12097  13564  11436  12600  10863   \n",
       "35  ...     8688  11077  10890      0  12337  13765   9125  12682   8701   \n",
       "36  ...    12432  12116  12097  12337      0  13567  12831  12397  12254   \n",
       "37  ...    13961  13634  13564  13765  13567      0  14368  13568  13768   \n",
       "38  ...     6978  11636  11436   9125  12831  14368      0  13225   9266   \n",
       "39  ...    12823  12483  12600  12682  12397  13568  13225      0  12612   \n",
       "40  ...     8800  11093  10863   8701  12254  13768   9266  12612      0   \n",
       "41  ...     8090  11129  10880   8655  12283  13789   8479  12733   8754   \n",
       "\n",
       "       41  \n",
       "0   10984  \n",
       "1    7922  \n",
       "2   10487  \n",
       "3    8993  \n",
       "4    9634  \n",
       "5    8016  \n",
       "6    8033  \n",
       "7   12188  \n",
       "8    7963  \n",
       "9   11093  \n",
       "10   8966  \n",
       "11  13349  \n",
       "12  10503  \n",
       "13   9644  \n",
       "14   8305  \n",
       "15   8255  \n",
       "16   8360  \n",
       "17  15306  \n",
       "18   8006  \n",
       "19   8069  \n",
       "20   8058  \n",
       "21  21475  \n",
       "22   8259  \n",
       "23   8020  \n",
       "24   8540  \n",
       "25  10823  \n",
       "26  12846  \n",
       "27   8182  \n",
       "28   8092  \n",
       "29  11204  \n",
       "30   7929  \n",
       "31   8334  \n",
       "32   8090  \n",
       "33  11129  \n",
       "34  10880  \n",
       "35   8655  \n",
       "36  12283  \n",
       "37  13789  \n",
       "38   8479  \n",
       "39  12733  \n",
       "40   8754  \n",
       "41      0  \n",
       "\n",
       "[42 rows x 42 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim = pd.DataFrame(index = range(0, pd.DataFrame(aux).shape[0]), columns = range(0,pd.DataFrame(aux).shape[0]))\n",
    "\n",
    "for i in np.arange(sim.shape[0]):\n",
    "    for j in np.arange(sim.shape[0]):\n",
    "        sim.iloc[i][j] = textdistance.hamming(pd.DataFrame(aux).iloc[i][0], pd.DataFrame(aux).iloc[j][0])\n",
    "    \n",
    "sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 18.3\n",
    "\n",
    "Create a classifier to predict the sex of each student"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are relevant the `stop_words` ? Keep on mind that the text are in Spanish and English.\n",
    "\n",
    "* The 10 most common words in English ([source](https://en.wikipedia.org/wiki/Most_common_words_in_English))\n",
    "* The 10 most common words in Spanish ([source](http://corpus.rae.es/frec/1000_formas.TXT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_stopwords_en = {'the','be','to','of','and','a','in','that','have','I'}\n",
    "list_stopwords_es = {'de','la','que','el','en','y','los','se','del','las'}\n",
    "\n",
    "list_stopwords = set.union(list_stopwords_en,list_stopwords_es)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, extract the features from the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>5857</th>\n",
       "      <th>5858</th>\n",
       "      <th>5859</th>\n",
       "      <th>5860</th>\n",
       "      <th>5861</th>\n",
       "      <th>5862</th>\n",
       "      <th>5863</th>\n",
       "      <th>5864</th>\n",
       "      <th>5865</th>\n",
       "      <th>5866</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5867 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0     1     2     3     4     5     6     7     8     9     ...   5857  \\\n",
       "0     0     0     0     1     0     0     0     0     1     1  ...      1   \n",
       "1     0     0     0     0     0     0     0     0     1     0  ...      0   \n",
       "2     0     0     0     0     0     0     0     0     1     0  ...      0   \n",
       "3     0     0     0     0     0     0     0     0     0     0  ...      0   \n",
       "4     0     0     0     0     0     0     0     0     0     0  ...      0   \n",
       "\n",
       "   5858  5859  5860  5861  5862  5863  5864  5865  5866  \n",
       "0     0     3     0     0     0     0     0     0     0  \n",
       "1     0     0     0     0     0     1     0     1     0  \n",
       "2     0     0     0     0     0     0     0     0     0  \n",
       "3     0     1     1     0     0     0     0     1     0  \n",
       "4     0     1     0     0     0     0     0     0     0  \n",
       "\n",
       "[5 rows x 5867 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(stop_words=list_stopwords)\n",
    "\n",
    "X_dtm = vect.fit_transform(aux).toarray()\n",
    "\n",
    "pd.DataFrame(X_dtm).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtained the vocabulary of the texts analyzed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'un': 5591,\n",
       " 'científico': 910,\n",
       " 'datos': 1522,\n",
       " 'ingeniero': 3076,\n",
       " 'tienen': 5469,\n",
       " 'ciertas': 914,\n",
       " 'habilidades': 2801,\n",
       " 'común': 1143,\n",
       " 'por': 4217,\n",
       " 'ejemplo': 2000,\n",
       " 'poseen': 4228,\n",
       " 'programación': 4445,\n",
       " 'sin': 5133,\n",
       " 'embargo': 2033,\n",
       " 'están': 2364,\n",
       " 'mucho': 3665,\n",
       " 'más': 3700,\n",
       " 'allá': 306,\n",
       " 'hacer': 2819,\n",
       " 'cree': 1425,\n",
       " 'flujo': 2602,\n",
       " 'está': 2363,\n",
       " 'extremo': 2528,\n",
       " 'alejado': 280,\n",
       " 'sus': 5335,\n",
       " 'pero': 4116,\n",
       " 'es': 2220,\n",
       " 'pan': 3980,\n",
       " 'cada': 763,\n",
       " 'día': 1952,\n",
       " 'general': 2713,\n",
       " 'existen': 2443,\n",
       " 'varias': 5706,\n",
       " 'diferencias': 1801,\n",
       " 'entre': 2173,\n",
       " 'puede': 4532,\n",
       " 'organizar': 3934,\n",
       " 'almacenar': 310,\n",
       " 'gran': 2769,\n",
       " 'cantidad': 805,\n",
       " 'además': 175,\n",
       " 'realizar': 4653,\n",
       " 'análisis': 389,\n",
       " 'descriptivos': 1689,\n",
       " 'predictivos': 4298,\n",
       " 'prescriptivos': 4333,\n",
       " 'con': 1145,\n",
       " 'fin': 2580,\n",
       " 'resolver': 4853,\n",
       " 'una': 5592,\n",
       " 'necesidad': 3737,\n",
       " 'negocio': 3749,\n",
       " 'generar': 2726,\n",
       " 'valor': 5683,\n",
       " 'compañía': 1067,\n",
       " 'dentro': 1615,\n",
       " 'áreas': 5841,\n",
       " 'conocimiento': 1218,\n",
       " 'matemáticas': 3477,\n",
       " 'estadística': 2306,\n",
       " 'física': 2684,\n",
       " 'investigación': 3204,\n",
       " 'operaciones': 3889,\n",
       " 'ciencias': 908,\n",
       " 'computación': 1127,\n",
       " 'responsable': 4868,\n",
       " 'optimizar': 3912,\n",
       " 'rendimiento': 4789,\n",
       " 'modelo': 3628,\n",
       " 'aprendizaje': 446,\n",
       " 'automático': 576,\n",
       " 'estadístico': 2309,\n",
       " 'lenguajes': 3296,\n",
       " 'usualmente': 5645,\n",
       " 'utiliza': 5654,\n",
       " 'son': 5196,\n",
       " 'python': 4550,\n",
       " 'sas': 4972,\n",
       " 'spss': 5223,\n",
       " 'matlab': 3482,\n",
       " 'julia': 3242,\n",
       " 'otros': 3955,\n",
       " 'también': 5364,\n",
       " 'acuerdo': 147,\n",
       " 'glassdoor': 2749,\n",
       " 'ingresos': 3092,\n",
       " 'anuales': 386,\n",
       " 'estados': 2305,\n",
       " 'unidos': 5600,\n",
       " '101': 3,\n",
       " '183': 25,\n",
       " 'mil': 3585,\n",
       " 'dólares': 1954,\n",
       " 'promedio': 4458,\n",
       " '140': 12,\n",
       " 'su': 5255,\n",
       " 'parte': 4026,\n",
       " 'construye': 1286,\n",
       " 'desarrolla': 1648,\n",
       " 'prueba': 4514,\n",
       " 'mantiene': 3449,\n",
       " 'arquitecturas': 486,\n",
       " 'tales': 5357,\n",
       " 'como': 1043,\n",
       " 'bases': 641,\n",
       " 'sistemas': 5146,\n",
       " 'procesamiento': 4403,\n",
       " 'larga': 3274,\n",
       " 'escala': 2223,\n",
       " 'responsabilidades': 4867,\n",
       " 'descubrir': 1695,\n",
       " 'oportunidades': 3900,\n",
       " 'para': 3991,\n",
       " 'adquisición': 200,\n",
       " 'desarrollar': 1657,\n",
       " 'procesos': 4411,\n",
       " 'unificación': 5601,\n",
       " 'modelamiento': 3622,\n",
       " 'minería': 3588,\n",
       " 'producción': 4417,\n",
       " 'recomendar': 4685,\n",
       " 'formas': 2626,\n",
       " 'mejorar': 3533,\n",
       " 'confianza': 1189,\n",
       " 'eficiencia': 1983,\n",
       " 'calidad': 781,\n",
       " 'requiere': 4834,\n",
       " 'conocimientos': 1219,\n",
       " 'middleware': 3579,\n",
       " 'hardware': 2837,\n",
       " 'óptimo': 5855,\n",
       " 'toda': 5480,\n",
       " 'tubería': 5562,\n",
       " 'herramientas': 2856,\n",
       " 'softwares': 5172,\n",
       " 'frecuentemente': 2640,\n",
       " 'sap': 4970,\n",
       " 'oracle': 3919,\n",
       " 'mysql': 3696,\n",
       " 'redis': 4711,\n",
       " 'riak': 4913,\n",
       " 'neo4j': 3753,\n",
       " 'mongodb': 3644,\n",
       " 'java': 3230,\n",
       " 'último': 5859,\n",
       " '123': 9,\n",
       " '170': 20,\n",
       " '151': 14,\n",
       " 'árboles': 5838,\n",
       " 'decisión': 1557,\n",
       " 'uno': 5611,\n",
       " 'algoritmos': 291,\n",
       " 'clasificación': 939,\n",
       " 'predicción': 4281,\n",
       " 'utilizados': 5659,\n",
       " 'corresponden': 1383,\n",
       " 'familia': 2546,\n",
       " 'supervisado': 5316,\n",
       " 'cart': 843,\n",
       " 'classification': 959,\n",
       " 'regression': 4757,\n",
       " 'tree': 5552,\n",
       " 'tipos': 5475,\n",
       " 'conocidos': 1217,\n",
       " 'resultado': 4881,\n",
       " 'clase': 929,\n",
       " 'cual': 1446,\n",
       " 'va': 5669,\n",
       " 'pertenecer': 4129,\n",
       " 'observación': 3837,\n",
       " 'mientras': 3583,\n",
       " 'número': 3825,\n",
       " 'real': 4639,\n",
       " 'precio': 4262,\n",
       " 'producto': 4423,\n",
       " 'bicicletas': 672,\n",
       " 'alquiladas': 314,\n",
       " 'rango': 4607,\n",
       " 'horario': 2887,\n",
       " 'tiempo': 5463,\n",
       " 'recuperación': 4700,\n",
       " 'enfermedad': 2101,\n",
       " 'árbol': 5837,\n",
       " 'estructura': 2350,\n",
       " 'similar': 5112,\n",
       " 'diagrama': 1778,\n",
       " 'donde': 1935,\n",
       " 'nodo': 3786,\n",
       " 'interno': 3171,\n",
       " 'representa': 4808,\n",
       " 'característica': 828,\n",
       " 'atributo': 544,\n",
       " 'variable': 5692,\n",
       " 'rama': 4595,\n",
       " 'regla': 4751,\n",
       " 'terminal': 5434,\n",
       " 'superior': 5310,\n",
       " 'conoce': 1210,\n",
       " 'raíz': 4626,\n",
       " 'funciona': 2659,\n",
       " 'siguiente': 5109,\n",
       " 'forma': 2617,\n",
       " 'primero': 4365,\n",
       " 'ubica': 5583,\n",
       " 'mejor': 3524,\n",
       " 'base': 639,\n",
       " 'segundo': 5015,\n",
       " 'subconjuntos': 5259,\n",
       " 'deberían': 1536,\n",
       " 'ser': 5054,\n",
       " 'creados': 1413,\n",
       " 'tal': 5356,\n",
       " 'contiene': 1314,\n",
       " 'información': 3066,\n",
       " 'mismo': 3608,\n",
       " 'deben': 1532,\n",
       " 'repetir': 4799,\n",
       " 'pasos': 4062,\n",
       " 'dos': 1936,\n",
       " 'subconjunto': 5258,\n",
       " 'hasta': 2841,\n",
       " 'llegue': 3359,\n",
       " 'todas': 5481,\n",
       " 'ramas': 4596,\n",
       " 'primer': 4362,\n",
       " 'reto': 4892,\n",
       " 'al': 260,\n",
       " 'implementar': 2954,\n",
       " 'identificar': 2917,\n",
       " 'cuál': 1488,\n",
       " 'debe': 1530,\n",
       " 'estar': 2321,\n",
       " 'nivel': 3778,\n",
       " 'esto': 2337,\n",
       " 'utilizan': 5660,\n",
       " 'métricas': 3711,\n",
       " 'saber': 4952,\n",
       " 'índice': 5849,\n",
       " 'gini': 2746,\n",
       " 'ganancia': 2692,\n",
       " 'estos': 2338,\n",
       " 'criterios': 1432,\n",
       " 'calcularán': 776,\n",
       " 'valores': 5685,\n",
       " 'ordenan': 3923,\n",
       " 'atributos': 545,\n",
       " 'colocan': 1005,\n",
       " 'siguiendo': 5108,\n",
       " 'orden': 3920,\n",
       " 'decir': 1550,\n",
       " 'alto': 325,\n",
       " 'caso': 850,\n",
       " 'coloca': 1004,\n",
       " 'particular': 4044,\n",
       " 'métrica': 3710,\n",
       " 'medir': 3521,\n",
       " 'frecuencia': 2638,\n",
       " 'elemento': 2017,\n",
       " 'elegido': 2013,\n",
       " 'azar': 606,\n",
       " 'identifica': 2912,\n",
       " 'incorrectamente': 3014,\n",
       " 'significa': 5093,\n",
       " 'preferirse': 4309,\n",
       " 'bajo': 622,\n",
       " 'respecto': 4861,\n",
       " 'aparte': 396,\n",
       " 'hay': 2842,\n",
       " 'otras': 3953,\n",
       " 'especificaciones': 2267,\n",
       " 'c4': 757,\n",
       " 'chaid': 893,\n",
       " 'inferencia': 3057,\n",
       " 'condicionales': 1168,\n",
       " 'generados': 2712,\n",
       " 'pueden': 4533,\n",
       " 'usados': 5622,\n",
       " 'esta': 2290,\n",
       " 'razón': 4624,\n",
       " 'casi': 849,\n",
       " 'siempre': 5088,\n",
       " 'referido': 4734,\n",
       " 'clasificador': 943,\n",
       " 'desde': 1696,\n",
       " 'grupo': 2782,\n",
       " 'entrenamiento': 2183,\n",
       " 'usando': 5625,\n",
       " 'concepto': 1148,\n",
       " 'entropía': 2195,\n",
       " 'observaciones': 3836,\n",
       " 'ya': 5832,\n",
       " 'clasificadas': 941,\n",
       " 'vector': 5719,\n",
       " 'características': 829,\n",
       " 'aumentados': 552,\n",
       " 'c1': 755,\n",
       " 'c2': 756,\n",
       " 'cn': 982,\n",
       " 'representan': 4815,\n",
       " 'pertenece': 4127,\n",
       " 'muestra': 3667,\n",
       " 'elige': 2021,\n",
       " 'eficazmente': 1982,\n",
       " 'divida': 1909,\n",
       " 'conjunto': 1206,\n",
       " 'muestras': 3670,\n",
       " 'enriquecidos': 2128,\n",
       " 'otra': 3952,\n",
       " 'criterio': 1431,\n",
       " 'normalizada': 3792,\n",
       " 'mayor': 3493,\n",
       " 'parámetro': 4053,\n",
       " 'chi': 900,\n",
       " 'squared': 5226,\n",
       " 'automatic': 568,\n",
       " 'interaction': 3149,\n",
       " 'detector': 1746,\n",
       " 'realiza': 4645,\n",
       " 'divisiones': 1918,\n",
       " 'multinivel': 3680,\n",
       " 'calcular': 775,\n",
       " 'usa': 5617,\n",
       " 'menudo': 3552,\n",
       " 'marketing': 3471,\n",
       " 'seleccionar': 5027,\n",
       " 'grupos': 2783,\n",
       " 'consumidores': 1293,\n",
       " 'predecir': 4275,\n",
       " 'cómo': 1497,\n",
       " 'respuestas': 4873,\n",
       " 'algunas': 296,\n",
       " 'variables': 5693,\n",
       " 'afectan': 206,\n",
       " 'aunque': 560,\n",
       " 'aplicaciones': 405,\n",
       " 'tempranas': 5399,\n",
       " 'fueron': 2652,\n",
       " 'campo': 798,\n",
       " 'médica': 3706,\n",
       " 'psiquiátrica': 4520,\n",
       " 'condicional': 1167,\n",
       " 'enfoque': 2110,\n",
       " 'pruebas': 4515,\n",
       " 'no': 3782,\n",
       " 'paramétricas': 4008,\n",
       " 'división': 1919,\n",
       " 'corregido': 1375,\n",
       " 'múltiples': 3718,\n",
       " 'evitar': 2413,\n",
       " 'sobreajuste': 5163,\n",
       " 'este': 2326,\n",
       " 'selecciona': 5022,\n",
       " 'predictores': 4302,\n",
       " 'insesgada': 3106,\n",
       " 'documento': 1926,\n",
       " 'do': 1921,\n",
       " 'we': 5795,\n",
       " 'need': 3743,\n",
       " 'hundreds': 2900,\n",
       " 'classifiers': 965,\n",
       " 'solve': 5188,\n",
       " 'world': 5820,\n",
       " 'problems': 4393,\n",
       " 'autores': 580,\n",
       " 'evalúan': 2400,\n",
       " '179': 23,\n",
       " 'clasificadores': 944,\n",
       " '17': 19,\n",
       " 'familias': 2550,\n",
       " '121': 8,\n",
       " 'implementados': 2951,\n",
       " 'weka': 5802,\n",
       " 'determinar': 1761,\n",
       " 'cuáles': 1489,\n",
       " 'llevar': 3364,\n",
       " 'cabo': 760,\n",
       " 'tarea': 5371,\n",
       " 'generan': 2724,\n",
       " 'training': 5526,\n",
       " 'test': 5441,\n",
       " 'set': 5077,\n",
       " 'aleatoria': 270,\n",
       " 'luego': 3396,\n",
       " 'parámetros': 4054,\n",
       " 'ajustados': 251,\n",
       " 'seleccionando': 5026,\n",
       " 'aquellos': 470,\n",
       " 'proporcionan': 4480,\n",
       " 'precisión': 4270,\n",
       " 'train': 5524,\n",
       " 'posteriormente': 4244,\n",
       " 'llevan': 3362,\n",
       " 'fold': 2605,\n",
       " 'cross': 1435,\n",
       " 'validation': 5677,\n",
       " 'utilizando': 5661,\n",
       " 'otro': 3954,\n",
       " 'lado': 3271,\n",
       " 'medidas': 3517,\n",
       " 'utilizaron': 5664,\n",
       " 'average': 593,\n",
       " 'accuracy': 103,\n",
       " 'friedman': 2644,\n",
       " 'ranking': 4612,\n",
       " 'cohen': 995,\n",
       " 'concluyen': 1161,\n",
       " 'mejores': 3537,\n",
       " 'bosques': 709,\n",
       " 'aleatorios': 275,\n",
       " 'seguidos': 5011,\n",
       " 'máquinas': 3699,\n",
       " 'vectores': 5720,\n",
       " 'soporte': 5202,\n",
       " '25': 44,\n",
       " 'caret': 832,\n",
       " 'librería': 3308,\n",
       " 'software': 5171,\n",
       " 'reportan': 4804,\n",
       " '82': 58,\n",
       " '30': 46,\n",
       " 'kernel': 3255,\n",
       " 'guassiano': 2790,\n",
       " 'implementadas': 2949,\n",
       " 'lenguaje': 3295,\n",
       " '81': 57,\n",
       " 'parallel': 4002,\n",
       " 'random': 4601,\n",
       " 'forest': 2612,\n",
       " 'parrf': 4024,\n",
       " 'ajusta': 249,\n",
       " 'mtry': 3661,\n",
       " 'debería': 1535,\n",
       " 'usado': 5621,\n",
       " 'referencia': 4730,\n",
       " 'incluídos': 3003,\n",
       " '20': 31,\n",
       " 'resultados': 4882,\n",
       " 'alcanzan': 267,\n",
       " 'controversial': 1345,\n",
       " 'sentido': 5044,\n",
       " 'ha': 2798,\n",
       " 'suscitado': 5336,\n",
       " 'diferentes': 1804,\n",
       " 'reacciones': 4632,\n",
       " 'dicen': 1782,\n",
       " 'hace': 2816,\n",
       " 'buen': 728,\n",
       " 'trabajo': 5514,\n",
       " 'cuantificar': 1457,\n",
       " 'impacto': 2937,\n",
       " 'ajustar': 255,\n",
       " 'consistentes': 1260,\n",
       " 'encontrados': 2081,\n",
       " 'numerosas': 3819,\n",
       " 'competiciones': 1075,\n",
       " 'kaggle': 3249,\n",
       " 'sugieren': 5296,\n",
       " 'lo': 3371,\n",
       " 'comunidad': 1142,\n",
       " 'académica': 90,\n",
       " 'le': 3278,\n",
       " 'podría': 4191,\n",
       " 'criticar': 1433,\n",
       " 'utilizado': 5658,\n",
       " 'sesgos': 5076,\n",
       " 'altos': 326,\n",
       " 'niveles': 3779,\n",
       " 'varianza': 5703,\n",
       " 'cuenta': 1464,\n",
       " 'partición': 4043,\n",
       " 'estándar': 2365,\n",
       " 'comparaciones': 1047,\n",
       " 'experimentos': 2471,\n",
       " 'imposible': 2973,\n",
       " 'eficiente': 1985,\n",
       " 'términos': 5578,\n",
       " 'computacionales': 1124,\n",
       " 'debido': 1537,\n",
       " 'complejidad': 1081,\n",
       " 'desconocida': 1675,\n",
       " 'posible': 4233,\n",
       " 'si': 5085,\n",
       " 'error': 2212,\n",
       " 'diseño': 1868,\n",
       " 'dificultades': 1813,\n",
       " 'intrínsecas': 3195,\n",
       " 'problema': 4391,\n",
       " 'cosas': 1394,\n",
       " 'destacables': 1723,\n",
       " 'encuentran': 2090,\n",
       " 'compara': 1044,\n",
       " 'usual': 5643,\n",
       " 'tipo': 5473,\n",
       " 'documentos': 1927,\n",
       " 'obtenidos': 3853,\n",
       " 'puedan': 4531,\n",
       " 'extrapolar': 2523,\n",
       " 'cosa': 1393,\n",
       " 'ocurre': 3868,\n",
       " 'cuando': 1453,\n",
       " 'mide': 3580,\n",
       " 'pequeño': 4087,\n",
       " 'última': 5857,\n",
       " 'sugerencia': 5293,\n",
       " 'sería': 5067,\n",
       " 'presentar': 4341,\n",
       " 'amigable': 334,\n",
       " 'intentar': 3140,\n",
       " 'modelos': 3630,\n",
       " 'cogiendo': 992,\n",
       " 'vez': 5746,\n",
       " 'fuerza': 2656,\n",
       " 'uso': 5640,\n",
       " 'desarrollos': 1662,\n",
       " 'recientes': 4674,\n",
       " 'deep': 1570,\n",
       " 'learning': 3285,\n",
       " 'boosting': 698,\n",
       " 'builds': 734,\n",
       " 'models': 3631,\n",
       " 'from': 2645,\n",
       " 'individual': 3044,\n",
       " 'so': 5159,\n",
       " 'called': 785,\n",
       " 'weak': 5796,\n",
       " 'learners': 3283,\n",
       " 'an': 343,\n",
       " 'iterative': 3224,\n",
       " 'way': 5793,\n",
       " 'are': 482,\n",
       " 'not': 3799,\n",
       " 'built': 735,\n",
       " 'on': 3883,\n",
       " 'completely': 1095,\n",
       " 'subsets': 5274,\n",
       " 'data': 1515,\n",
       " 'features': 2568,\n",
       " 'but': 746,\n",
       " 'sequentially': 5053,\n",
       " 'by': 747,\n",
       " 'putting': 4549,\n",
       " 'more': 3647,\n",
       " 'weight': 5798,\n",
       " 'instances': 3113,\n",
       " 'with': 5815,\n",
       " 'wrong': 5822,\n",
       " 'predictions': 4292,\n",
       " 'high': 2863,\n",
       " 'errors': 2214,\n",
       " 'idea': 2908,\n",
       " 'behind': 660,\n",
       " 'this': 5453,\n",
       " 'is': 3218,\n",
       " 'which': 5807,\n",
       " 'hard': 2836,\n",
       " 'predict': 4289,\n",
       " 'correctly': 1372,\n",
       " 'difficult': 1810,\n",
       " 'cases': 848,\n",
       " 'will': 5814,\n",
       " 'focused': 2604,\n",
       " 'during': 1946,\n",
       " 'model': 3618,\n",
       " 'learns': 3287,\n",
       " 'past': 4063,\n",
       " 'mistakes': 3611,\n",
       " 'when': 5805,\n",
       " 'each': 1956,\n",
       " 'ensemble': 2144,\n",
       " 'subset': 5273,\n",
       " 'also': 317,\n",
       " 'call': 784,\n",
       " 'stochastic': 5245,\n",
       " 'gradient': 2761,\n",
       " 'can': 801,\n",
       " 'help': 2852,\n",
       " 'improve': 2977,\n",
       " 'generalizability': 2716,\n",
       " 'our': 3957,\n",
       " 'used': 5635,\n",
       " 'minimize': 3597,\n",
       " 'loss': 3392,\n",
       " 'function': 2669,\n",
       " 'how': 2890,\n",
       " 'neural': 3762,\n",
       " 'nets': 3759,\n",
       " 'utilize': 5666,\n",
       " 'descent': 1671,\n",
       " 'optimize': 3915,\n",
       " 'learn': 3281,\n",
       " 'weights': 5801,\n",
       " 'round': 4936,\n",
       " 'learner': 3282,\n",
       " 'its': 3227,\n",
       " 'compared': 1057,\n",
       " 'correct': 1367,\n",
       " 'outcome': 3959,\n",
       " 'expect': 2459,\n",
       " 'distance': 1888,\n",
       " 'between': 667,\n",
       " 'prediction': 4291,\n",
       " 'truth': 5559,\n",
       " 'represents': 4824,\n",
       " 'rate': 4619,\n",
       " 'these': 5451,\n",
       " 'now': 3804,\n",
       " 'calculate': 777,\n",
       " 'nothing': 3801,\n",
       " 'fancy': 2555,\n",
       " 'it': 3219,\n",
       " 'basically': 643,\n",
       " 'partial': 4029,\n",
       " 'derivative': 1641,\n",
       " 'describes': 1680,\n",
       " 'steepness': 5242,\n",
       " 'find': 2588,\n",
       " 'direction': 1836,\n",
       " 'change': 895,\n",
       " 'parameters': 4004,\n",
       " 'order': 3926,\n",
       " 'maximally': 3486,\n",
       " 'reduce': 4713,\n",
       " 'next': 3768,\n",
       " 'descending': 1669,\n",
       " 'both': 712,\n",
       " 'xgboost': 5827,\n",
       " 'gbm': 2704,\n",
       " 'follows': 2609,\n",
       " 'principle': 4373,\n",
       " 'there': 5450,\n",
       " 'however': 2891,\n",
       " 'difference': 1806,\n",
       " 'modeling': 3627,\n",
       " 'details': 1734,\n",
       " 'specifically': 5215,\n",
       " 'regularized': 4766,\n",
       " 'formalization': 2623,\n",
       " 'control': 1337,\n",
       " 'over': 3963,\n",
       " 'fitting': 2595,\n",
       " 'gives': 2748,\n",
       " 'better': 666,\n",
       " 'performance': 4102,\n",
       " 'name': 3724,\n",
       " 'though': 5455,\n",
       " 'actually': 142,\n",
       " 'refers': 4735,\n",
       " 'engineering': 2119,\n",
       " 'goal': 2753,\n",
       " 'push': 4548,\n",
       " 'limit': 3320,\n",
       " 'computations': 1134,\n",
       " 'resources': 4856,\n",
       " 'for': 2611,\n",
       " 'boosted': 696,\n",
       " 'algorithms': 287,\n",
       " 'reason': 4662,\n",
       " 'why': 5810,\n",
       " 'many': 3452,\n",
       " 'people': 4083,\n",
       " 'use': 5634,\n",
       " 'might': 3584,\n",
       " 'suitable': 5297,\n",
       " 'as': 501,\n",
       " 'addition': 164,\n",
       " 'developers': 1771,\n",
       " 'made': 3411,\n",
       " 'number': 3815,\n",
       " 'important': 2968,\n",
       " 'enhancements': 2123,\n",
       " 'different': 1808,\n",
       " 'parts': 4052,\n",
       " 'implementation': 2955,\n",
       " 'make': 3421,\n",
       " 'big': 676,\n",
       " 'speed': 5216,\n",
       " 'memory': 3539,\n",
       " 'utilization': 5665,\n",
       " 'sparse': 5213,\n",
       " 'matrices': 3483,\n",
       " 'sparsity': 5214,\n",
       " 'aware': 597,\n",
       " 'algorithms2': 288,\n",
       " 'improved': 2978,\n",
       " 'structures': 5253,\n",
       " 'processor': 4414,\n",
       " 'cache': 761,\n",
       " 'makes': 3422,\n",
       " 'faster': 2558,\n",
       " 'support': 5326,\n",
       " 'multicore': 3679,\n",
       " 'processing': 4413,\n",
       " 'reduces': 4715,\n",
       " 'overall': 3964,\n",
       " 'time': 5472,\n",
       " 'other': 3950,\n",
       " 'words': 5818,\n",
       " 'designed': 1714,\n",
       " 'scale': 4978,\n",
       " 'initial': 3101,\n",
       " 'perfiles': 4101,\n",
       " 'cuentan': 1465,\n",
       " 'comunes': 1139,\n",
       " 'significativas': 5101,\n",
       " 'scientist': 4982,\n",
       " 'engineer': 2118,\n",
       " 'fuerte': 2653,\n",
       " 'formación': 2618,\n",
       " 'construcción': 1278,\n",
       " 'destaca': 1722,\n",
       " 'administración': 190,\n",
       " 'grandes': 2771,\n",
       " 'volúmenes': 5774,\n",
       " 'distribuidos': 1899,\n",
       " 'hablar': 2812,\n",
       " 'tiene': 5468,\n",
       " 'encargarse': 2070,\n",
       " 'construir': 1284,\n",
       " 'mantener': 3444,\n",
       " 'preparación': 4325,\n",
       " 'distribución': 1896,\n",
       " 'garantizando': 2696,\n",
       " 'confiabilidad': 1186,\n",
       " 'ideando': 2910,\n",
       " 'nuevas': 3810,\n",
       " 'automatizar': 573,\n",
       " 'estas': 2324,\n",
       " 'tareas': 5372,\n",
       " 'herramienta': 2855,\n",
       " 'adecuada': 167,\n",
       " 'actividad': 133,\n",
       " 'ese': 2247,\n",
       " 'rol': 4929,\n",
       " 'servir': 5064,\n",
       " 'apoyo': 427,\n",
       " 'todos': 5483,\n",
       " 'posibles': 4235,\n",
       " 'clientes': 971,\n",
       " 'esas': 2222,\n",
       " 'fuentes': 2649,\n",
       " 'incluyendo': 3002,\n",
       " 'encarga': 2065,\n",
       " 'elaborar': 2007,\n",
       " 'analizar': 354,\n",
       " 'proporcionar': 4481,\n",
       " 'valiosa': 5679,\n",
       " 'hallazgos': 2831,\n",
       " 'presentados': 4338,\n",
       " 'comprensible': 1117,\n",
       " 'partes': 4028,\n",
       " 'involucradas': 3209,\n",
       " 'toma': 5485,\n",
       " 'decisiones': 1554,\n",
       " 'habilidad': 2800,\n",
       " 'caracteriza': 824,\n",
       " 'hechas': 2849,\n",
       " 'mano': 3441,\n",
       " 'especializado': 2265,\n",
       " 'poder': 4181,\n",
       " 'abordar': 80,\n",
       " 'obtención': 3846,\n",
       " 'limpieza': 3336,\n",
       " 'elaboración': 2006,\n",
       " 'informes': 3068,\n",
       " 'gráficos': 2787,\n",
       " 'permitan': 4107,\n",
       " 'comunicar': 1141,\n",
       " 'conceso': 1152,\n",
       " 'total': 5501,\n",
       " 'tema': 5395,\n",
       " 'manera': 3437,\n",
       " 'machine': 3407,\n",
       " 'dependiendo': 1625,\n",
       " 'propósito': 4492,\n",
       " 'superviado': 5313,\n",
       " 'supervised': 5318,\n",
       " 'algoritmo': 290,\n",
       " 'resulta': 4880,\n",
       " 'fácil': 2681,\n",
       " 'entender': 2153,\n",
       " 'aplicar': 413,\n",
       " 'popular': 4213,\n",
       " 'función': 2668,\n",
       " 'deduce': 1568,\n",
       " 'partir': 4050,\n",
       " 'históricos': 2875,\n",
       " 'etiquetados': 2377,\n",
       " 'sirven': 5144,\n",
       " 'asociar': 526,\n",
       " 'entrada': 2167,\n",
       " 'respuesta': 4872,\n",
       " 'esperada': 2280,\n",
       " 'sido': 5087,\n",
       " 'aprendida': 441,\n",
       " 'previamente': 4354,\n",
       " 'completado': 1091,\n",
       " 'capaz': 816,\n",
       " 'completamente': 1092,\n",
       " 'nuevos': 3812,\n",
       " 'siguientes': 5110,\n",
       " 'vecinos': 5718,\n",
       " 'cercanos': 884,\n",
       " 'bayes': 651,\n",
       " 'arboles': 477,\n",
       " 'regresión': 4756,\n",
       " 'lineal': 3337,\n",
       " 'redes': 4709,\n",
       " 'neuronales': 3764,\n",
       " 'destacadas': 1724,\n",
       " 'reconocimiento': 4692,\n",
       " 'imágenes': 2983,\n",
       " 'publicidad': 4524,\n",
       " 'sugerida': 5294,\n",
       " 'correo': 1380,\n",
       " 'deseado': 1700,\n",
       " 'etiquetar': 2380,\n",
       " 'autónoma': 581,\n",
       " 'encontrando': 2083,\n",
       " 'patrones': 4068,\n",
       " 'útil': 5865,\n",
       " 'encontrar': 2084,\n",
       " 'segmentar': 5007,\n",
       " 'naturalmente': 3727,\n",
       " 'encuentra': 2089,\n",
       " 'etiquetada': 2374,\n",
       " 'escenario': 2233,\n",
       " 'muy': 3695,\n",
       " 'altamente': 319,\n",
       " 'aplicado': 408,\n",
       " 'segmentación': 5006,\n",
       " 'mercado': 3554,\n",
       " 'basado': 634,\n",
       " 'consumo': 1294,\n",
       " 'semi': 5036,\n",
       " 'combinación': 1014,\n",
       " 'previos': 4357,\n",
       " 'reforzado': 4744,\n",
       " 'sistema': 5145,\n",
       " 'simula': 5127,\n",
       " 'premios': 4322,\n",
       " 'castigos': 854,\n",
       " 'ambiente': 330,\n",
       " 'controlado': 1340,\n",
       " 'orientado': 3939,\n",
       " 'agente': 217,\n",
       " 'comienzo': 1038,\n",
       " 'cometerá': 1035,\n",
       " 'muchos': 3666,\n",
       " 'errores': 2213,\n",
       " 'dependiedo': 1624,\n",
       " 'estímulo': 2368,\n",
       " 'ajustará': 256,\n",
       " 'learing': 3280,\n",
       " 'métodos': 3709,\n",
       " 'ensamblaje': 2135,\n",
       " 'combinar': 1021,\n",
       " 'obtener': 3849,\n",
       " 'usar': 5626,\n",
       " 'único': 5863,\n",
       " 'principio': 4371,\n",
       " 'detrás': 1768,\n",
       " 'método': 3708,\n",
       " 'suma': 5298,\n",
       " 'débiles': 1950,\n",
       " 'combinarse': 1023,\n",
       " 'ensamblarse': 2139,\n",
       " 'producirán': 4422,\n",
       " 'técnicas': 5574,\n",
       " 'bagging': 619,\n",
       " 'principal': 4367,\n",
       " 'nuevo': 3811,\n",
       " 'escogiendo': 2241,\n",
       " 'reemplazo': 4727,\n",
       " 'registros': 4750,\n",
       " 'colección': 998,\n",
       " 'entrenará': 2190,\n",
       " 'obtiene': 3855,\n",
       " 'entrenados': 2182,\n",
       " 'contener': 1305,\n",
       " 'versiones': 5737,\n",
       " 'predicciones': 4280,\n",
       " 'resultará': 4887,\n",
       " 'robusto': 4925,\n",
       " 'solo': 5180,\n",
       " 'extensión': 2512,\n",
       " 'baggin': 618,\n",
       " 'agrega': 223,\n",
       " 'paso': 4061,\n",
       " 'extra': 2519,\n",
       " 'selección': 5029,\n",
       " 'lugar': 3397,\n",
       " 'original': 3945,\n",
       " 'combinaciones': 1013,\n",
       " 'ventaja': 5729,\n",
       " 'desempeño': 1709,\n",
       " 'ante': 374,\n",
       " 've': 5714,\n",
       " 'afectada': 204,\n",
       " 'datasets': 1519,\n",
       " 'contengan': 1307,\n",
       " 'faltantes': 2545,\n",
       " 'técnica': 5573,\n",
       " 'crear': 1416,\n",
       " 'reduzca': 4723,\n",
       " 'sesgo': 5074,\n",
       " 'existe': 2442,\n",
       " 'bosting': 711,\n",
       " 'catorias': 869,\n",
       " 'binarias': 681,\n",
       " 'categorización': 859,\n",
       " 'multiclase': 3677,\n",
       " 'paper': 3985,\n",
       " 'publicado': 4522,\n",
       " '2014': 38,\n",
       " 'evaluación': 2390,\n",
       " 'svm': 5341,\n",
       " 'lineales': 3338,\n",
       " 'multinomial': 3681,\n",
       " 'logística': 3390,\n",
       " 'componentes': 1106,\n",
       " 'principales': 4368,\n",
       " 'ellos': 2032,\n",
       " 'plataformas': 4160,\n",
       " 'sobre': 5160,\n",
       " 'mayoría': 3500,\n",
       " 'uci': 5587,\n",
       " 'repository': 4806,\n",
       " 'unos': 5612,\n",
       " 'problemas': 4392,\n",
       " 'vida': 5748,\n",
       " 'variada': 5696,\n",
       " 'buscó': 742,\n",
       " 'eliminar': 2028,\n",
       " 'adicionalmente': 181,\n",
       " 'usó': 5648,\n",
       " 'misma': 3606,\n",
       " 'ejercicio': 2003,\n",
       " 'autor': 579,\n",
       " 'concluye': 1160,\n",
       " 'parrf_t': 4025,\n",
       " 'implementado': 2950,\n",
       " 'consigue': 1253,\n",
       " '94': 66,\n",
       " 'máxima': 3702,\n",
       " 'lograda': 3379,\n",
       " 'pama': 3979,\n",
       " 'siglas': 5091,\n",
       " 'inglés': 3082,\n",
       " 'ajustado': 250,\n",
       " 'ligeramente': 3316,\n",
       " 'inferior': 3058,\n",
       " '93': 65,\n",
       " 'tercer': 5430,\n",
       " 'implementación': 2947,\n",
       " 'libsvm': 3312,\n",
       " 'ajustando': 254,\n",
       " 'spread': 5222,\n",
       " 'logra': 3378,\n",
       " '92': 64,\n",
       " 'tener': 5409,\n",
       " 'seis': 5020,\n",
       " 'cinco': 918,\n",
       " 'top': 5498,\n",
       " 'final': 2581,\n",
       " 'considera': 1236,\n",
       " 'proponerse': 4476,\n",
       " 'golden': 2757,\n",
       " 'basarse': 638,\n",
       " 'propuestos': 4489,\n",
       " 'classifiergradient': 964,\n",
       " 'busca': 736,\n",
       " 'estimador': 2332,\n",
       " 'estimadores': 2333,\n",
       " 'generlamente': 2735,\n",
       " 'decision': 1552,\n",
       " 'trees': 5553,\n",
       " 'optimización': 3908,\n",
       " 'pérdida': 4553,\n",
       " 'optimiza': 3906,\n",
       " 'iterativamente': 3223,\n",
       " 'coste': 1396,\n",
       " 'estimación': 2330,\n",
       " 'fue': 2647,\n",
       " 'inicialmente': 3097,\n",
       " 'desarrollado': 1651,\n",
       " 'leo': 3300,\n",
       " 'breiman': 717,\n",
       " 'quien': 4573,\n",
       " 'hizo': 2877,\n",
       " 'aporte': 424,\n",
       " 'inicia': 3093,\n",
       " 'proceso': 4410,\n",
       " 'débil': 1949,\n",
       " 'incorpora': 3008,\n",
       " 'iteración': 3221,\n",
       " 'reducir': 4720,\n",
       " 'residual': 4847,\n",
       " 'diferencia': 1794,\n",
       " 'actualizar': 141,\n",
       " 'incorporando': 3012,\n",
       " ...}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize the features for the ML model for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['average_Words'] = preprocessing.scale(results['average_Words'])\n",
    "results['average_Character'] = preprocessing.scale(results['average_Words'])\n",
    "results['Missing_values'] = preprocessing.scale(results['average_Words'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the features dataframe and target dataframe for the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>5861</th>\n",
       "      <th>5862</th>\n",
       "      <th>5863</th>\n",
       "      <th>5864</th>\n",
       "      <th>5865</th>\n",
       "      <th>5866</th>\n",
       "      <th>average_Words</th>\n",
       "      <th>average_Characters</th>\n",
       "      <th>Missing_values</th>\n",
       "      <th>average_Character</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.547085</td>\n",
       "      <td>2893.500000</td>\n",
       "      <td>1.547085</td>\n",
       "      <td>1.547085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.659080</td>\n",
       "      <td>1702.600000</td>\n",
       "      <td>-0.659080</td>\n",
       "      <td>-0.659080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.308629</td>\n",
       "      <td>1850.333333</td>\n",
       "      <td>-0.308629</td>\n",
       "      <td>-0.308629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.330252</td>\n",
       "      <td>1917.200000</td>\n",
       "      <td>-0.330252</td>\n",
       "      <td>-0.330252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.498767</td>\n",
       "      <td>1706.833333</td>\n",
       "      <td>-0.498767</td>\n",
       "      <td>-0.498767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5871 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2  3  4  5  6  7  8  9        ...          5861  5862  5863  5864  \\\n",
       "0  0  0  0  1  0  0  0  0  1  1        ...             0     0     0     0   \n",
       "1  0  0  0  0  0  0  0  0  1  0        ...             0     0     1     0   \n",
       "2  0  0  0  0  0  0  0  0  1  0        ...             0     0     0     0   \n",
       "3  0  0  0  0  0  0  0  0  0  0        ...             0     0     0     0   \n",
       "4  0  0  0  0  0  0  0  0  0  0        ...             0     0     0     0   \n",
       "\n",
       "   5865  5866  average_Words  average_Characters  Missing_values  \\\n",
       "0     0     0       1.547085         2893.500000        1.547085   \n",
       "1     1     0      -0.659080         1702.600000       -0.659080   \n",
       "2     0     0      -0.308629         1850.333333       -0.308629   \n",
       "3     1     0      -0.330252         1917.200000       -0.330252   \n",
       "4     0     0      -0.498767         1706.833333       -0.498767   \n",
       "\n",
       "   average_Character  \n",
       "0           1.547085  \n",
       "1          -0.659080  \n",
       "2          -0.308629  \n",
       "3          -0.330252  \n",
       "4          -0.498767  \n",
       "\n",
       "[5 rows x 5871 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ = pd.concat([pd.DataFrame(X_dtm), results], axis=1, sort=False)\n",
    "\n",
    "X_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['Sexo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=10, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
       "            oob_score=False, random_state=42, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_jobs=-1,\n",
    "                             n_estimators=100,\n",
    "                             max_depth=10, \n",
    "                             random_state=42)\n",
    "\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9090909090909091"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result is because the `RandomForestClassifier` creates everything as a `'H'` since the dataset is unbalanced for `'H'`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
